# Grading API

Evaluation and scoring functionality for agent submissions on biomedical tasks.

::: biomlbench.grade
    options:
      show_source: false
      heading_level: 2

::: biomlbench.grade_helpers
    options:
      show_source: false
      heading_level: 2

## Medal System

BioML-bench uses a Kaggle-style medal system that varies based on leaderboard size:

**For small leaderboards (1-99 teams):**
- **ğŸ¥‡ Gold**: Top 10% of submissions
- **ğŸ¥ˆ Silver**: Top 20% (but not gold)  
- **ğŸ¥‰ Bronze**: Top 40% (but not silver/gold)

**For medium leaderboards (100-249 teams):**
- **ğŸ¥‡ Gold**: Top 10 positions (fixed)
- **ğŸ¥ˆ Silver**: Top 20% (but not gold)
- **ğŸ¥‰ Bronze**: Top 40% (but not silver/gold)

**For large leaderboards (250-999 teams):**
- **ğŸ¥‡ Gold**: Top (10 + 0.2% of teams) positions
- **ğŸ¥ˆ Silver**: Top 50 positions (fixed)
- **ğŸ¥‰ Bronze**: Top 100 positions (fixed)

**For very large leaderboards (1000+ teams):**
- **ğŸ¥‡ Gold**: Top (10 + 0.2% of teams) positions
- **ğŸ¥ˆ Silver**: Top 5% of submissions
- **ğŸ¥‰ Bronze**: Top 10% of submissions

Medal thresholds follow the official Kaggle competition progression system.

## Usage Examples

### Single Task Evaluation

```python
from biomlbench.grade import grade_csv
from biomlbench.registry import registry

# Grade a single submission
task = registry.get_task("caco2-wang")
submission_path = Path("submission.csv")

report = grade_csv(submission_path, task)

print(f"Score: {report.score}")
print(f"Medal: {'ğŸ¥‡' if report.gold_medal else 'ğŸ¥ˆ' if report.silver_medal else 'ğŸ¥‰' if report.bronze_medal else 'âŒ'}")
print(f"Beats human: {report.beats_human}")
```

### Multi-Task Evaluation

```python
from biomlbench.grade import grade_jsonl

# Grade multiple tasks from submission.jsonl
grade_jsonl(
    path_to_submissions=Path("runs/my-run-group/submission.jsonl"),
    output_dir=Path("results/")
)
``` 