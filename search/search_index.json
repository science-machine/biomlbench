{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BioML-bench","text":"<p>A benchmark suite for evaluating LLM agents on biomedical machine learning tasks.</p> <p>BioML-bench is built on top of MLE-bench and provides a comprehensive framework for benchmarking LLM agents on biomedical machine learning (BioML) tasks including protein engineering, drug discovery, medical imaging, and clinical biomarkers.</p> <p>Agents autonomously read task descriptions, analyze biomedical data, design appropriate ML approaches, and implement complete solutions from scratch.</p>"},{"location":"#features","title":"\ud83e\uddec Features","text":"<ul> <li>Diverse Biomedical Tasks: Protein engineering, drug discovery, medical imaging, clinical biomarkers</li> <li>Agent-Agnostic Evaluation: Any LLM agent that can read task descriptions and produce CSV submissions can be evaluated</li> <li>Human Baselines: Built-in human performance benchmarks for comparison</li> <li>Secure Evaluation: Containerized execution with no data leakage</li> <li>Extensible Framework: Easy to add new biomedical tasks</li> <li>Biomedical Libraries: Pre-installed RDKit, BioPython, and other domain-specific tools</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Install the package (requires uv):</p> <pre><code># Install with uv\ngit clone https://github.com/science-machine/biomlbench.git\ncd biomlbench\nuv sync\n</code></pre> <p>Build and run a benchmark with an agent:</p> <pre><code># Prepare a task\nbiomlbench prepare -t caco2-wang\n\n# Run an agent (in this case, a dummy agent that returns null predictions)\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# Grade results (submission.jsonl is auto-generated)\nbiomlbench grade --submission &lt;run-group-dir&gt;/submission.jsonl --output-dir results/\n</code></pre>"},{"location":"#available-tasks","title":"\ud83d\udcca Available Tasks","text":""},{"location":"#medical-imaging","title":"Medical Imaging","text":"<ul> <li>histopathologic-cancer-detection: Cancer detection in histopathology patches</li> </ul>"},{"location":"#drug-discovery","title":"Drug Discovery","text":"<ul> <li>caco2-wang: Molecular property prediction (intestinal permeability)</li> </ul>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>BioML-bench follows a modular architecture:</p> <ul> <li>Core Framework (<code>biomlbench/</code>) - Task management, grading, data handling</li> <li>Agent System (<code>agents/</code>) - Agent registry and execution framework  </li> <li>Environment (<code>environment/</code>) - Containerized execution environment</li> <li>Tasks (<code>biomlbench/tasks/</code>) - Individual biomedical benchmark tasks</li> <li>Scripts (<code>scripts/</code>) - Build, test, and deployment automation</li> </ul>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":"<ul> <li>Quick Start - Get up and running</li> <li>API Reference - Complete API documentation</li> <li>Components - Deep dive into system components</li> <li>Developer Guide - Extend the framework</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! See our Contributing Guide for details on:</p> <ul> <li>Adding new biomedical tasks</li> <li>Creating custom agents</li> <li>Extending data sources</li> <li>Improving documentation </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers installing BioML-bench and setting up the environment for running agents on biomedical tasks.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Docker - For containerized agent execution</li> <li>uv - Python package manager (recommended)</li> </ul>"},{"location":"installation/#install-uv","title":"Install uv","text":"<p>BioML-bench uses uv for fast dependency management:</p> <pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or with pip\npip install uv\n</code></pre>"},{"location":"installation/#install-docker","title":"Install Docker","text":"<p>Docker is required for secure agent execution:</p> Ubuntu/DebianmacOSWindows <pre><code>sudo apt update\nsudo apt install docker.io\nsudo systemctl start docker\nsudo usermod -aG docker $USER\n# Log out and back in for group changes\n</code></pre> <pre><code># Install Docker Desktop from https://docker.com/products/docker-desktop\n# Or with Homebrew\nbrew install --cask docker\n</code></pre> <p>Download and install Docker Desktop.</p>"},{"location":"installation/#installing-bioml-bench","title":"Installing BioML-bench","text":""},{"location":"installation/#from-source-recommended-for-development","title":"From Source (Recommended for Development)","text":"<pre><code># Clone the repository\ngit clone https://github.com/science-machine/biomlbench.git\ncd biomlbench\n\n# Install dependencies with uv\nuv sync\n\n# Activate the virtual environment\nsource .venv/bin/activate  # Linux/macOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Verify installation\nbiomlbench --help\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For contributing to BioML-bench:</p> <pre><code># Clone and install in development mode\ngit clone https://github.com/science-machine/biomlbench.git\ncd biomlbench\n\n# Install with development dependencies\nuv sync --extra dev\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"installation/#environment-setup","title":"Environment Setup","text":""},{"location":"installation/#build-base-docker-environment","title":"Build Base Docker Environment","text":"<p>Before running agents, build the base Docker environment with biomedical libraries:</p> <pre><code># Build the biomlbench-env image\n./scripts/build_base_env.sh\n\n# Verify the environment\n./scripts/test_environment.sh\n</code></pre>"},{"location":"installation/#configuration","title":"Configuration","text":""},{"location":"installation/#polaris-api-for-polaris-based-tasks","title":"Polaris API (for polaris-based tasks)","text":"<pre><code>polaris login --overwrite\n</code></pre>"},{"location":"installation/#kaggle-api-for-kaggle-based-tasks","title":"Kaggle API (For Kaggle-based Tasks)","text":"<p>Some tasks require Kaggle data. Set up Kaggle API credentials:</p> <pre><code># Download API credentials from https://www.kaggle.com/account\n# Place in ~/.kaggle/kaggle.json (Linux/macOS) or %USERPROFILE%\\.kaggle\\kaggle.json (Windows)\n\n# Set permissions (Linux/macOS only)\nchmod 600 ~/.kaggle/kaggle.json\n</code></pre>"},{"location":"installation/#agent-api-keys","title":"Agent API Keys","text":"<p>For agents that require API access (e.g., AIDE):</p> <pre><code># Create environment file\necho \"OPENAI_API_KEY=your-key-here\" &gt;&gt; .env\necho \"ANTHROPIC_API_KEY=your-key-here\" &gt;&gt; .env\necho \"GEMINI_API_KEY=your-key-here\" &gt;&gt; .env\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code># Check CLI is working\nbiomlbench --help\n\n# List available tasks\nbiomlbench prepare --help\n\n# Test with dummy agent (requires Docker)\nbiomlbench prepare -t caco2-wang\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n</code></pre>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<ul> <li>Open an issue on GitHub </li> </ul>"},{"location":"usage/","title":"Basic Usage","text":"<p>This guide covers the basic workflow for using BioML-bench to evaluate agents on biomedical tasks.</p>"},{"location":"usage/#overview","title":"Overview","text":"<p>The typical BioML-bench workflow involves:</p> <ol> <li>Prepare task datasets</li> <li>Run agents on tasks  </li> <li>Grade agent submissions</li> <li>Analyze results</li> </ol>"},{"location":"usage/#task-preparation","title":"Task Preparation","text":"<p>Before running agents, you need to prepare task datasets:</p>"},{"location":"usage/#prepare-individual-tasks","title":"Prepare Individual Tasks","text":"<pre><code># Prepare a specific task\nbiomlbench prepare -t caco2-wang\n\n# Prepare with options\nbiomlbench prepare -t histopathologic-cancer-detection --keep-raw\n</code></pre>"},{"location":"usage/#prepare-multiple-tasks-filtered-tasks","title":"Prepare Multiple Tasks / filtered tasks","text":"<p>Not implemented yet.</p>"},{"location":"usage/#running-agents","title":"Running Agents","text":""},{"location":"usage/#built-in-agents","title":"Built-in Agents","text":"<p>BioML-bench includes several reference agents:</p> <pre><code># Dummy agent (for testing)\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# AIDE agent (requires OpenAI API key)\nbiomlbench run-agent --agent aide --task-id caco2-wang\n</code></pre>"},{"location":"usage/#single-task-execution","title":"Single Task Execution","text":"<pre><code># Run agent on one task\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# With custom settings\nbiomlbench run-agent \\\n    --agent dummy \\\n    --task-id caco2-wang \\\n    --n-seeds 3 \\\n    --retain-container\n</code></pre>"},{"location":"usage/#multi-task-execution-not-tested","title":"Multi-Task Execution (NOT TESTED)","text":"<pre><code># Run on multiple tasks from a list (example with available split)\nbiomlbench run-agent --agent dummy --task-list experiments/splits/caco2-wang.txt\n\n# Parallel execution (future: when more split files are available)\n# biomlbench run-agent \\\n#     --agent dummy \\\n#     --task-list experiments/splits/medical-imaging.txt \\\n#     --n-workers 4 \\\n#     --n-seeds 2\n</code></pre>"},{"location":"usage/#agent-execution-options","title":"Agent Execution Options","text":"<ul> <li><code>--n-workers</code>: Number of parallel workers (default: 1)</li> <li><code>--n-seeds</code>: Random seeds per task (default: 1)  </li> <li><code>--retain-container</code>: Keep containers for debugging</li> <li><code>--container-config</code>: Custom Docker configuration</li> <li><code>--data-dir</code>: Custom data directory</li> </ul>"},{"location":"usage/#understanding-agent-outputs","title":"Understanding Agent Outputs","text":"<p>When an agent run completes, BioML-bench creates:</p> <pre><code>runs/\n\u2514\u2500\u2500 2024-01-15T10-30-00-GMT_run-group_dummy/\n    \u251c\u2500\u2500 metadata.json              # Run summary\n    \u251c\u2500\u2500 submission.jsonl           # Auto-generated submission file\n    \u251c\u2500\u2500 caco2-wang_abc123/         # Individual task run\n    \u2502   \u251c\u2500\u2500 submission/\n    \u2502   \u2502   \u2514\u2500\u2500 submission.csv     # Agent predictions\n    \u2502   \u251c\u2500\u2500 logs/\n    \u2502   \u2502   \u2514\u2500\u2500 run.log           # Execution logs\n    \u2502   \u2514\u2500\u2500 code/                 # Agent code (if available)\n    \u2514\u2500\u2500 task2_def456/             # Additional task runs...\n</code></pre>"},{"location":"usage/#key-files","title":"Key Files","text":"<ul> <li><code>metadata.json</code>: Run metadata and success status</li> <li><code>submission.jsonl</code>: Automatically generated submission index</li> <li><code>submission.csv</code>: Agent's predictions for each task</li> <li><code>run.log</code>: Detailed execution logs</li> </ul>"},{"location":"usage/#grading-and-evaluation","title":"Grading and Evaluation","text":""},{"location":"usage/#grade-agent-runs","title":"Grade Agent Runs","text":"<p>The submission file is automatically generated after agent execution:</p> <pre><code># Grade using auto-generated submission file\nbiomlbench grade \\\n    --submission runs/2024-01-15T10-30-00-GMT_run-group_dummy/submission.jsonl \\\n    --output-dir results/\n</code></pre>"},{"location":"usage/#grade-individual-submissions","title":"Grade Individual Submissions","text":"<p>For testing or external submissions:</p> <pre><code># Grade a single CSV file\nbiomlbench grade-sample submission.csv caco2-wang\n</code></pre>"},{"location":"usage/#baseline-comparisons","title":"Baseline Comparisons","text":"<p>These baselines don't really work yet.</p> <p>Run baselines to establish performance benchmarks:</p> <pre><code># Run specific baseline\nbiomlbench run-baseline caco2-wang --baseline linear\n\n# Run all available baselines\nbiomlbench run-baseline caco2-wang --baseline all\n\n# Grade baseline results\nbiomlbench grade --submission baseline_submissions/submission.jsonl --output-dir results/\n</code></pre>"},{"location":"usage/#working-with-results","title":"Working with Results","text":""},{"location":"usage/#understanding-scores","title":"Understanding Scores","text":"<p>Each task uses task/domain-specific metrics.</p> <p>E.g., Caco2-Wang uses MAE, while Histopathologic Cancer Detection uses AUC-ROC.</p>"},{"location":"usage/#medal-system","title":"Medal System","text":"<p>BioML-bench uses a Kaggle-style medal system that varies based on leaderboard size:</p> <p>For small leaderboards (1-99 teams): - \ud83e\udd47 Gold: Top 10% of submissions - \ud83e\udd48 Silver: Top 20% (but not gold) - \ud83e\udd49 Bronze: Top 40% (but not silver/gold)</p> <p>For medium leaderboards (100-249 teams): - \ud83e\udd47 Gold: Top 10 positions (fixed) - \ud83e\udd48 Silver: Top 20% (but not gold) - \ud83e\udd49 Bronze: Top 40% (but not silver/gold)</p> <p>For large leaderboards (250-999 teams): - \ud83e\udd47 Gold: Top (10 + 0.2% of teams) positions - \ud83e\udd48 Silver: Top 50 positions (fixed) - \ud83e\udd49 Bronze: Top 100 positions (fixed)</p> <p>For very large leaderboards (1000+ teams): - \ud83e\udd47 Gold: Top (10 + 0.2% of teams) positions - \ud83e\udd48 Silver: Top 5% of submissions - \ud83e\udd49 Bronze: Top 10% of submissions</p> <p>This follows the official Kaggle competition progression system.</p>"},{"location":"usage/#human-baseline-comparison","title":"Human Baseline Comparison","text":"<p>Many tasks include human expert performance for context:</p> <pre><code>{\n  \"task_id\": \"histopathologic-cancer-detection\",\n  \"score\": 0.89,\n  \"beats_human\": true,\n  \"human_percentile\": 85.5\n}\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/#custom-container-configuration","title":"Custom Container Configuration","text":"<p>Override Docker settings for specific requirements:</p> <pre><code>// custom_config.json\n{\n    \"mem_limit\": \"8g\",\n    \"shm_size\": \"4g\", \n    \"nano_cpus\": 4000000000,\n    \"gpus\": -1\n}\n</code></pre> <pre><code>biomlbench run-agent \\\n    --agent my-agent \\\n    --task-id caco2-wang \\\n    --container-config custom_config.json\n</code></pre>"},{"location":"usage/#task-development-workflow","title":"Task Development Workflow","text":"<p>For full details, see developer/adding_tasks.md</p> <pre><code># 1. Create new task structure\nmkdir -p biomlbench/tasks/my-new-task\n\n# 2. Test task preparation\nbiomlbench prepare -t my-new-task\n\n# 3. Test with dummy agent\nbiomlbench run-agent --agent dummy --task-id my-new-task\n\n# 4. Grade test submission\nbiomlbench grade-sample runs/dummy/my-new-task/submission.csv my-new-task\n</code></pre>"},{"location":"usage/#cli-reference","title":"CLI Reference","text":""},{"location":"usage/#main-commands","title":"Main Commands","text":"<ul> <li><code>prepare</code>: Download and prepare task datasets</li> <li><code>run-agent</code>: Execute agents on tasks</li> <li><code>grade</code>: Evaluate submissions  </li> <li><code>grade-sample</code>: Test individual submissions</li> <li><code>run-baseline</code>: Generate baseline comparisons</li> </ul>"},{"location":"usage/#getting-help","title":"Getting Help","text":"<pre><code># Main help\nbiomlbench --help\n\n# Command-specific help\nbiomlbench prepare --help\nbiomlbench run-agent --help\nbiomlbench grade --help\n</code></pre>"},{"location":"usage/#common-workflows","title":"Common Workflows","text":""},{"location":"usage/#quick-testing","title":"Quick Testing","text":"<pre><code># Fast workflow for testing\nbiomlbench prepare -t caco2-wang\nbiomlbench run-agent --agent dummy --task-id caco2-wang\nbiomlbench grade --submission runs/*/submission.jsonl --output-dir results/\n</code></pre>"},{"location":"usage/#full-evaluation","title":"Full Evaluation","text":"<pre><code># Comprehensive evaluation (when more tasks are available)\n# biomlbench prepare --all\n# biomlbench run-agent --agent my-agent --task-list experiments/splits/all.txt --n-workers 4\n# biomlbench grade --submission runs/*/submission.jsonl --output-dir results/\n\n# Current example with available tasks\nbiomlbench prepare -t caco2-wang -t histopathologic-cancer-detection\nbiomlbench run-agent --agent my-agent --task-id caco2-wang\nbiomlbench grade --submission runs/*/submission.jsonl --output-dir results/\n</code></pre>"},{"location":"api/agents/","title":"Agents API","text":"<p>Agent execution and management functionality for running LLM agents on biomedical tasks.</p> <p>Agent execution module for BioML-bench.</p> <p>This module provides functionality to run AI agents on biomedical tasks within the biomlbench framework.</p>"},{"location":"api/agents/#biomlbench.agents.AgentTask","title":"AgentTask  <code>dataclass</code>","text":"<pre><code>AgentTask(run_id, seed, image, path_to_run_group, path_to_run, agent, task, container_config)\n</code></pre> <p>Represents a single agent-task execution.</p>"},{"location":"api/agents/#biomlbench.agents.worker","title":"worker  <code>async</code>","text":"<pre><code>worker(idx, queue, client, tasks_outputs, retain_container=False)\n</code></pre> <p>Worker function that processes agent tasks from the queue.</p>"},{"location":"api/agents/#biomlbench.agents.create_task_list_file","title":"create_task_list_file","text":"<pre><code>create_task_list_file(task_id)\n</code></pre> <p>Create a temporary task list file for single task execution.</p>"},{"location":"api/agents/#biomlbench.agents.get_task_ids","title":"get_task_ids","text":"<pre><code>get_task_ids(task_id=None, task_list=None)\n</code></pre> <p>Get list of task IDs from either single task or task list file.</p>"},{"location":"api/agents/#biomlbench.agents.run_agent_async","title":"run_agent_async  <code>async</code>","text":"<pre><code>run_agent_async(agent_id, task_ids, n_workers=1, n_seeds=1, container_config_path=None, retain_container=False, data_dir=None)\n</code></pre> <p>Run an agent on multiple tasks asynchronously.</p> <p>Returns:     Tuple[str, Path]: The run group ID and path to the generated submission file</p>"},{"location":"api/agents/#biomlbench.agents.run_agent","title":"run_agent","text":"<pre><code>run_agent(args)\n</code></pre> <p>Main entry point for running agents from the CLI.</p> <p>Args:     args: Parsed command line arguments</p> <p>Returns:     str: The run group ID for this execution</p>"},{"location":"api/cli/","title":"CLI Reference","text":"<p>BioML-bench's command-line interface for managing biomedical benchmark tasks and running agent evaluations.</p>"},{"location":"api/cli/#main-help","title":"Main Help","text":"<pre><code>usage: cli.py [-h] {prepare,grade,grade-sample,dev,run-baseline,run-agent} ...\n\nRuns agents on biomedical ML tasks.\n\npositional arguments:\n  {prepare,grade,grade-sample,dev,run-baseline,run-agent}\n                        Sub-command to run.\n    prepare             Download and prepare tasks for the BioML-bench\n                        dataset.\n    grade               Grade a submission to the eval, comprising of several\n                        task submissions\n    grade-sample        Grade a single sample (task) in the eval\n    dev                 Developer tools for extending BioML-bench.\n    run-baseline        Run a baseline agent on a biomedical task\n    run-agent           Run an AI agent on biomedical tasks\n\noptions:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"api/cli/#commands","title":"Commands","text":""},{"location":"api/cli/#biomlbench-prepare","title":"<code>biomlbench prepare</code>","text":"<pre><code>usage: cli.py prepare [-h] [-t TASK_ID] [-a] [--lite] [-l LIST]\n                      [--domain DOMAIN] [--task-type TASK_TYPE] [--keep-raw]\n                      [--data-dir DATA_DIR] [--overwrite-checksums]\n                      [--overwrite-leaderboard] [--skip-verification]\n\noptions:\n  -h, --help            show this help message and exit\n  -t TASK_ID, --task-id TASK_ID\n                        ID of the task to prepare. Valid options:\n                        ['caco2-wang', 'histopathologic-cancer-detection']\n  -a, --all             Prepare all tasks.\n  --lite                Prepare all the low difficulty tasks (BioML-bench\n                        Lite).\n  -l LIST, --list LIST  Prepare a list of tasks specified line by line in a\n                        text file.\n  --domain DOMAIN       Prepare all tasks for a specific biomedical domain\n                        (e.g., oncology, drug_discovery).\n  --task-type TASK_TYPE\n                        Prepare all tasks of a specific type (e.g.,\n                        medical_imaging, protein_engineering).\n  --keep-raw            Keep the raw task files after the task has been\n                        prepared.\n  --data-dir DATA_DIR   Path to the directory where the data will be stored.\n  --overwrite-checksums\n                        [For Developers] Overwrite the checksums file for the\n                        task.\n  --overwrite-leaderboard\n                        [For Developers] Overwrite the leaderboard file for\n                        the task.\n  --skip-verification   [For Developers] Skip the verification of the\n                        checksums.\n</code></pre>"},{"location":"api/cli/#biomlbench-run-agent","title":"<code>biomlbench run-agent</code>","text":"<pre><code>usage: cli.py run-agent [-h] --agent AGENT [--task-id TASK_ID]\n                        [--task-list TASK_LIST] [--n-workers N_WORKERS]\n                        [--n-seeds N_SEEDS]\n                        [--container-config CONTAINER_CONFIG]\n                        [--retain-container] [--data-dir DATA_DIR]\n                        [--output-dir OUTPUT_DIR]\n\noptions:\n  -h, --help            show this help message and exit\n  --agent AGENT         Agent ID to run (e.g., dummy, aide, aide/dev)\n  --task-id TASK_ID     Single task ID to run. Valid options: ['caco2-wang',\n                        'histopathologic-cancer-detection']\n  --task-list TASK_LIST\n                        Path to text file with task IDs (one per line) for\n                        multi-task runs\n  --n-workers N_WORKERS\n                        Number of parallel workers for multi-task runs\n  --n-seeds N_SEEDS     Number of random seeds to run per task\n  --container-config CONTAINER_CONFIG\n                        Path to JSON file with Docker container configuration\n  --retain-container    Keep container after run for debugging\n  --data-dir DATA_DIR   Path to the directory where task data is stored\n  --output-dir OUTPUT_DIR\n                        Directory to save agent run outputs\n</code></pre>"},{"location":"api/cli/#biomlbench-grade","title":"<code>biomlbench grade</code>","text":"<pre><code>usage: cli.py grade [-h] --submission SUBMISSION --output-dir OUTPUT_DIR\n                    [--data-dir DATA_DIR]\n\noptions:\n  -h, --help            show this help message and exit\n  --submission SUBMISSION\n                        Path to the JSONL file of submissions. Refer to\n                        README.md#submission-format for the required format.\n  --output-dir OUTPUT_DIR\n                        Path to the directory where the evaluation metrics\n                        will be saved.\n  --data-dir DATA_DIR   Path to the directory where the data used for grading\n                        is stored.\n</code></pre>"},{"location":"api/cli/#biomlbench-grade-sample","title":"<code>biomlbench grade-sample</code>","text":"<pre><code>usage: cli.py grade-sample [-h] [--data-dir DATA_DIR] submission task_id\n\npositional arguments:\n  submission           Path to the submission CSV file.\n  task_id              ID of the task to grade. Valid options: ['caco2-wang',\n                       'histopathologic-cancer-detection']\n\noptions:\n  -h, --help           show this help message and exit\n  --data-dir DATA_DIR  Path to the directory where the data will be stored.\n</code></pre>"},{"location":"api/cli/#biomlbench-run-baseline","title":"<code>biomlbench run-baseline</code>","text":"<pre><code>usage: cli.py run-baseline [-h] [--baseline BASELINE]\n                           [--output-dir OUTPUT_DIR] [--data-dir DATA_DIR]\n                           [--seed SEED]\n                           task_id\n\npositional arguments:\n  task_id               ID of the task to run baseline on. Valid options:\n                        ['caco2-wang', 'histopathologic-cancer-detection']\n\noptions:\n  -h, --help            show this help message and exit\n  --baseline BASELINE   Baseline to run (simple, random, or task-specific\n                        types like linear, rf, fingerprint). Use 'all' to run\n                        all available baselines for the task.\n  --output-dir OUTPUT_DIR\n                        Directory to save baseline submissions\n  --data-dir DATA_DIR   Path to the directory where the data is stored.\n  --seed SEED           Random seed for reproducible baselines\n</code></pre>"},{"location":"api/cli/#biomlbench-dev","title":"<code>biomlbench dev</code>","text":"<pre><code>usage: cli.py dev [-h] {download-leaderboard,prepare-human-baselines} ...\n\npositional arguments:\n  {download-leaderboard,prepare-human-baselines}\n                        Developer command to run.\n    download-leaderboard\n                        Download the leaderboard for a task.\n    prepare-human-baselines\n                        Prepare human baseline data for tasks.\n\noptions:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"api/cli/#biomlbench-dev-download-leaderboard","title":"<code>biomlbench dev download-leaderboard</code>","text":"<pre><code>usage: cli.py dev download-leaderboard [-h] [-t TASK_ID] [--all] [--force]\n\noptions:\n  -h, --help            show this help message and exit\n  -t TASK_ID, --task-id TASK_ID\n                        Name of the task to download the leaderboard for.\n                        Valid options: ['caco2-wang', 'histopathologic-cancer-\n                        detection']\n  --all                 Download the leaderboard for all tasks.\n  --force               Force download the leaderboard, even if it already\n                        exists.\n</code></pre>"},{"location":"api/cli/#biomlbench-dev-prepare-human-baselines","title":"<code>biomlbench dev prepare-human-baselines</code>","text":"<pre><code>usage: cli.py dev prepare-human-baselines [-h] [-t TASK_ID] [--all] [--force]\n\noptions:\n  -h, --help            show this help message and exit\n  -t TASK_ID, --task-id TASK_ID\n                        Name of the task to prepare human baselines for. Valid\n                        options: ['caco2-wang', 'histopathologic-cancer-\n                        detection']\n  --all                 Prepare human baselines for all tasks.\n  --force               Force re-extraction of human baselines, even if they\n                        already exist.\n</code></pre>"},{"location":"api/cli/#usage-examples","title":"Usage Examples","text":""},{"location":"api/cli/#task-preparation","title":"Task Preparation","text":"<pre><code># Prepare a specific task\nbiomlbench prepare -t caco2-wang\n\n# Prepare all tasks in a domain\nbiomlbench prepare --domain admet\n\n# Prepare multiple tasks from a file\nbiomlbench prepare --list experiments/splits/caco2-wang.txt\n\n# Prepare all low-difficulty tasks\nbiomlbench prepare --lite\n</code></pre>"},{"location":"api/cli/#agent-execution","title":"Agent Execution","text":"<pre><code># Run agent on single task\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# Run agent on multiple tasks with parallel workers\nbiomlbench run-agent \\\n    --agent aide \\\n    --task-list experiments/splits/caco2-wang.txt \\\n    --n-workers 4 \\\n    --n-seeds 3\n\n# Run with custom container configuration\nbiomlbench run-agent \\\n    --agent aide \\\n    --task-id caco2-wang \\\n    --container-config custom_config.json \\\n    --retain-container\n</code></pre>"},{"location":"api/cli/#evaluation-and-grading","title":"Evaluation and Grading","text":"<pre><code># Grade multiple task submissions\nbiomlbench grade \\\n    --submission runs/my-run-group/submission.jsonl \\\n    --output-dir results/\n\n# Grade single task submission\nbiomlbench grade-sample submission.csv caco2-wang\n\n# Run and grade baselines\nbiomlbench run-baseline caco2-wang --baseline all\nbiomlbench grade \\\n    --submission baseline_submissions/submission.jsonl \\\n    --output-dir baseline_results/\n</code></pre>"},{"location":"api/cli/#development-commands","title":"Development Commands","text":"<pre><code># Download leaderboards for all tasks\nbiomlbench dev download-leaderboard --all\n\n# Prepare human baselines for a specific task\nbiomlbench dev prepare-human-baselines -t caco2-wang --force\n</code></pre>"},{"location":"api/cli/#environment-variables","title":"Environment Variables","text":"<p>BioML-bench respects these environment variables:</p>"},{"location":"api/cli/#agent-configuration","title":"Agent Configuration","text":"<ul> <li><code>OPENAI_API_KEY</code> - API key for AIDE agent</li> <li><code>I_ACCEPT_RUNNING_PRIVILEGED_CONTAINERS</code> - Set to \"true\" to allow privileged containers</li> </ul>"},{"location":"api/cli/#kaggle-integration","title":"Kaggle Integration","text":"<p>Kaggle authentication uses the standard configuration file at <code>~/.kaggle/kaggle.json</code>. See the Kaggle API documentation for setup instructions.</p>"},{"location":"api/cli/#common-patterns","title":"Common Patterns","text":""},{"location":"api/cli/#full-workflow-example","title":"Full Workflow Example","text":"<pre><code># 1. Prepare tasks\nbiomlbench prepare --lite\n\n# 2. Run agent\nbiomlbench run-agent --agent dummy --task-list experiments/splits/caco2-wang.txt\n\n# 3. Grade results (submission.jsonl is auto-generated)\nbiomlbench grade \\\n    --submission runs/latest-run-group/submission.jsonl \\\n    --output-dir results/\n</code></pre>"},{"location":"api/cli/#debugging-agent-issues","title":"Debugging Agent Issues","text":"<pre><code># Run with container retention for debugging\nbiomlbench run-agent \\\n    --agent my-agent \\\n    --task-id caco2-wang \\\n    --retain-container\n\n# Check logs in the run directory\nls runs/latest-run-group/*/logs/\n</code></pre>"},{"location":"api/data_sources/","title":"Data Sources API","text":"<p>Data source integration for downloading datasets from external platforms.</p> <p>Data sources package for BioML-bench.</p> <p>This package provides pluggable data source implementations for different platforms including Kaggle, Polaris, HTTP, and local files.</p> <p>The factory pattern allows easy registration and creation of data sources based on configuration.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSource","title":"DataSource","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data sources.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSource.download","title":"download  <code>abstractmethod</code>","text":"<pre><code>download(source_config, data_dir)\n</code></pre> <p>Download data from the source.</p> <p>Args:     source_config: Configuration specific to this data source     data_dir: Directory to download data to</p> <p>Returns:     Path to downloaded data (or None if no single file)</p> <p>Raises:     DataSourceError: If download fails</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSource.get_leaderboard","title":"get_leaderboard  <code>abstractmethod</code>","text":"<pre><code>get_leaderboard(source_config)\n</code></pre> <p>Get the public leaderboard for the benchmark.</p> <p>Args:     source_config: Configuration specific to this data source</p> <p>Returns:     DataFrame with columns: teamName, score, submissionDate</p> <p>Raises:     DataSourceError: If leaderboard cannot be retrieved</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSource.get_human_baselines","title":"get_human_baselines","text":"<pre><code>get_human_baselines(source_config)\n</code></pre> <p>Extract human baseline performance data.</p> <p>Args:     source_config: Configuration specific to this data source</p> <p>Returns:     DataFrame with columns: team_name, score, human_type, source     Returns None if no human baselines available</p> <p>Raises:     DataSourceError: If human baseline extraction fails</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSource.supports_human_baselines","title":"supports_human_baselines","text":"<pre><code>supports_human_baselines()\n</code></pre> <p>Check if this data source can provide human baseline data.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSourceError","title":"DataSourceError","text":"<pre><code>DataSourceError(message, source_type=None)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised for data source related errors.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSourceConfigError","title":"DataSourceConfigError","text":"<pre><code>DataSourceConfigError(message, source_type=None)\n</code></pre> <p>               Bases: <code>DataSourceError</code></p> <p>Exception raised when data source configuration is invalid.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSourceNotFoundError","title":"DataSourceNotFoundError","text":"<pre><code>DataSourceNotFoundError(message, source_type=None)\n</code></pre> <p>               Bases: <code>DataSourceError</code></p> <p>Exception raised when a data source type is not found.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSourceFactory","title":"DataSourceFactory","text":"<p>Factory for creating data source instances.</p> <p>Automatically discovers and registers data source classes, then creates instances based on source type configuration.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSourceFactory.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(source_type, source_class)\n</code></pre> <p>Register a data source class.</p> <p>Args:     source_type: String identifier for the source type     source_class: Data source class to register</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSourceFactory.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(source_type)\n</code></pre> <p>Create a data source instance.</p> <p>Args:     source_type: Type of data source to create</p> <p>Returns:     Configured data source instance</p> <p>Raises:     DataSourceNotFoundError: If source type is not registered</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSourceFactory.list_available","title":"list_available  <code>classmethod</code>","text":"<pre><code>list_available()\n</code></pre> <p>List all available data source types.</p> <p>Returns:     List of registered source type strings</p>"},{"location":"api/data_sources/#biomlbench.data_sources.DataSourceFactory.is_available","title":"is_available  <code>classmethod</code>","text":"<pre><code>is_available(source_type)\n</code></pre> <p>Check if a data source type is available.</p> <p>Args:     source_type: Source type to check</p> <p>Returns:     True if source type is registered</p>"},{"location":"api/data_sources/#biomlbench.data_sources.KaggleDataSource","title":"KaggleDataSource","text":"<p>               Bases: <code>DataSource</code></p> <p>Data source for Kaggle competitions.</p> <p>Downloads competition data and leaderboards using the Kaggle API.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.KaggleDataSource.validate_config","title":"validate_config","text":"<pre><code>validate_config(source_config)\n</code></pre> <p>Validate Kaggle source configuration.</p> <p>Args:     source_config: Should contain 'competition_id' key</p> <p>Returns:     True if valid</p> <p>Raises:     DataSourceConfigError: If configuration is invalid</p>"},{"location":"api/data_sources/#biomlbench.data_sources.KaggleDataSource.download","title":"download","text":"<pre><code>download(source_config, data_dir)\n</code></pre> <p>Download competition data from Kaggle.</p> <p>Args:     source_config: Must contain 'competition_id'     data_dir: Directory to download data to</p> <p>Returns:     Path to downloaded zip file</p> <p>Raises:     DataSourceError: If download fails</p>"},{"location":"api/data_sources/#biomlbench.data_sources.KaggleDataSource.get_leaderboard","title":"get_leaderboard","text":"<pre><code>get_leaderboard(source_config)\n</code></pre> <p>Get leaderboard from Kaggle competition.</p> <p>Args:     source_config: Must contain 'competition_id'</p> <p>Returns:     DataFrame with leaderboard data</p> <p>Raises:     DataSourceError: If leaderboard cannot be retrieved</p>"},{"location":"api/data_sources/#biomlbench.data_sources.KaggleDataSource.get_human_baselines","title":"get_human_baselines","text":"<pre><code>get_human_baselines(source_config)\n</code></pre> <p>Extract human baselines from Kaggle public leaderboard.</p> <p>Filters the public leaderboard to identify likely human participants and categorizes them by performance level.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.KaggleDataSource.supports_human_baselines","title":"supports_human_baselines","text":"<pre><code>supports_human_baselines()\n</code></pre> <p>Kaggle supports human baseline extraction from public leaderboards.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.PolarisDataSource","title":"PolarisDataSource","text":"<pre><code>PolarisDataSource()\n</code></pre> <p>               Bases: <code>DataSource</code></p> <p>Data source for Polaris Hub benchmarks.</p> <p>Downloads benchmark data and provides leaderboard information from the Polaris platform using the polarishub conda environment.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.PolarisDataSource.validate_config","title":"validate_config","text":"<pre><code>validate_config(source_config)\n</code></pre> <p>Validate Polaris source configuration.</p> <p>Args:     source_config: Should contain 'benchmark_id' key</p> <p>Returns:     True if valid</p> <p>Raises:     DataSourceConfigError: If configuration is invalid</p>"},{"location":"api/data_sources/#biomlbench.data_sources.PolarisDataSource.download","title":"download","text":"<pre><code>download(source_config, data_dir)\n</code></pre> <p>Download benchmark data from Polaris Hub.</p> <p>Args:     source_config: Must contain 'benchmark_id'     data_dir: Directory to save data to</p> <p>Returns:     Path to the data directory (Polaris doesn't use zip files)</p> <p>Raises:     DataSourceError: If download fails</p>"},{"location":"api/data_sources/#biomlbench.data_sources.PolarisDataSource.get_leaderboard","title":"get_leaderboard","text":"<pre><code>get_leaderboard(source_config)\n</code></pre> <p>Get leaderboard from Polaris Hub.</p> <p>Note: Polaris doesn't provide direct leaderboard access via API. This returns a minimal leaderboard structure.</p> <p>Args:     source_config: Must contain 'benchmark_id'</p> <p>Returns:     DataFrame with minimal leaderboard data</p> <p>Raises:     DataSourceError: If leaderboard cannot be created</p>"},{"location":"api/data_sources/#biomlbench.data_sources.register_data_source","title":"register_data_source","text":"<pre><code>register_data_source(source_type)\n</code></pre> <p>Decorator to automatically register data source classes.</p> <p>Args:     source_type: String identifier for the source type</p>"},{"location":"api/data_sources/#biomlbench.data_sources.list_available_sources","title":"list_available_sources","text":"<pre><code>list_available_sources()\n</code></pre> <p>Convenience function to list all available data source types.</p> <p>Returns:     List of registered data source type strings</p>"},{"location":"api/data_sources/#biomlbench.data_sources.create_data_source","title":"create_data_source","text":"<pre><code>create_data_source(source_type)\n</code></pre> <p>Convenience function to create a data source instance.</p> <p>Args:     source_type: Type of data source to create</p> <p>Returns:     Configured data source instance</p> <p>Raises:     DataSourceNotFoundError: If source type is not registered</p>"},{"location":"api/grading/","title":"Grading API","text":"<p>Evaluation and scoring functionality for agent submissions on biomedical tasks.</p> <p>High-level grading functionality</p> <p>Helper classes related to grading</p>"},{"location":"api/grading/#biomlbench.grade.grade_jsonl","title":"grade_jsonl","text":"<pre><code>grade_jsonl(path_to_submissions, output_dir, registry=DEFAULT_REGISTRY)\n</code></pre> <p>Grades multiple submissions stored in a JSONL file. Saves the aggregated report as a JSON file.</p>"},{"location":"api/grading/#biomlbench.grade.grade_csv","title":"grade_csv","text":"<pre><code>grade_csv(path_to_submission, task)\n</code></pre> <p>Grades a submission CSV for the given task.</p>"},{"location":"api/grading/#biomlbench.grade.validate_submission","title":"validate_submission","text":"<pre><code>validate_submission(submission, task)\n</code></pre> <p>Validates a submission for the given task by actually running the task grader. This is designed for end users, not developers (we assume that the task grader is correctly implemented and use that for validating the submission, not the other way around).</p>"},{"location":"api/grading/#biomlbench.grade.aggregate_reports","title":"aggregate_reports","text":"<pre><code>aggregate_reports(task_reports)\n</code></pre> <p>Builds the summary report from a list of task reports.</p>"},{"location":"api/grading/#biomlbench.grade.calculate_human_performance_metrics","title":"calculate_human_performance_metrics","text":"<pre><code>calculate_human_performance_metrics(agent_score, human_df, is_lower_better)\n</code></pre> <p>Calculate how an agent's performance compares to human baselines.</p> <p>Args:     agent_score: The agent's score to compare     human_df: DataFrame with human baseline scores (must have 'score' column)     is_lower_better: Whether lower scores are better for this metric</p> <p>Returns:     Tuple of (beats_human, human_percentile) where:     - beats_human: True if agent beats median human performance     - human_percentile: Percentile of human performance that agent achieves (0-100)</p>"},{"location":"api/grading/#biomlbench.grade_helpers.Grader","title":"Grader","text":"<pre><code>Grader(name, grade_fn)\n</code></pre>"},{"location":"api/grading/#biomlbench.grade_helpers.Grader.is_lower_better","title":"is_lower_better","text":"<pre><code>is_lower_better(leaderboard)\n</code></pre> <p>Determines if a lower score is better based on the leaderboard. Returns True if lower scores are better, False otherwise.</p>"},{"location":"api/grading/#biomlbench.grade_helpers.Grader.rank_score","title":"rank_score","text":"<pre><code>rank_score(score, leaderboard)\n</code></pre> <p>Ranks a score based on the leaderboard. Returns a dictionary of bools with the following keys: - gold_medal: bool - silver_medal: bool - bronze_medal: bool - above_median: bool - gold_threshold: float - silver_threshold: float - bronze_threshold: float - median_threshold: float</p>"},{"location":"api/grading/#biomlbench.grade_helpers.TaskReport","title":"TaskReport  <code>dataclass</code>","text":"<pre><code>TaskReport(task_id, score, gold_threshold, silver_threshold, bronze_threshold, median_threshold, any_medal, gold_medal, silver_medal, bronze_medal, above_median, submission_exists, valid_submission, is_lower_better, created_at, submission_path, beats_human=None, human_percentile=None)\n</code></pre> <p>Report for a single biomedical task evaluation.</p> <p>Extended from MLE-bench CompetitionReport with biomedical-specific fields.</p>"},{"location":"api/grading/#biomlbench.grade_helpers.InvalidSubmissionError","title":"InvalidSubmissionError","text":"<p>               Bases: <code>Exception</code></p> <p>A custom exception for when the agent submission cannot be graded.</p>"},{"location":"api/grading/#medal-system","title":"Medal System","text":"<p>BioML-bench uses a Kaggle-style medal system that varies based on leaderboard size:</p> <p>For small leaderboards (1-99 teams): - \ud83e\udd47 Gold: Top 10% of submissions - \ud83e\udd48 Silver: Top 20% (but not gold) - \ud83e\udd49 Bronze: Top 40% (but not silver/gold)</p> <p>For medium leaderboards (100-249 teams): - \ud83e\udd47 Gold: Top 10 positions (fixed) - \ud83e\udd48 Silver: Top 20% (but not gold) - \ud83e\udd49 Bronze: Top 40% (but not silver/gold)</p> <p>For large leaderboards (250-999 teams): - \ud83e\udd47 Gold: Top (10 + 0.2% of teams) positions - \ud83e\udd48 Silver: Top 50 positions (fixed) - \ud83e\udd49 Bronze: Top 100 positions (fixed)</p> <p>For very large leaderboards (1000+ teams): - \ud83e\udd47 Gold: Top (10 + 0.2% of teams) positions - \ud83e\udd48 Silver: Top 5% of submissions - \ud83e\udd49 Bronze: Top 10% of submissions</p> <p>Medal thresholds follow the official Kaggle competition progression system.</p>"},{"location":"api/grading/#usage-examples","title":"Usage Examples","text":""},{"location":"api/grading/#single-task-evaluation","title":"Single Task Evaluation","text":"<pre><code>from biomlbench.grade import grade_csv\nfrom biomlbench.registry import registry\n\n# Grade a single submission\ntask = registry.get_task(\"caco2-wang\")\nsubmission_path = Path(\"submission.csv\")\n\nreport = grade_csv(submission_path, task)\n\nprint(f\"Score: {report.score}\")\nprint(f\"Medal: {'\ud83e\udd47' if report.gold_medal else '\ud83e\udd48' if report.silver_medal else '\ud83e\udd49' if report.bronze_medal else '\u274c'}\")\nprint(f\"Beats human: {report.beats_human}\")\n</code></pre>"},{"location":"api/grading/#multi-task-evaluation","title":"Multi-Task Evaluation","text":"<pre><code>from biomlbench.grade import grade_jsonl\n\n# Grade multiple tasks from submission.jsonl\ngrade_jsonl(\n    path_to_submissions=Path(\"runs/my-run-group/submission.jsonl\"),\n    output_dir=Path(\"results/\")\n)\n</code></pre>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>This section provides comprehensive API documentation for BioML-bench's Python modules and classes.</p>"},{"location":"api/overview/#core-modules","title":"Core Modules","text":""},{"location":"api/overview/#cli-biomlbenchcli","title":"CLI - <code>biomlbench.cli</code>","text":"<p>Command-line interface and argument parsing for all BioML-bench commands.</p>"},{"location":"api/overview/#registry-biomlbenchregistry","title":"Registry - <code>biomlbench.registry</code>","text":"<p>Task registry system for discovering, loading, and managing biomedical benchmark tasks.</p>"},{"location":"api/overview/#tasks-biomlbenchdata","title":"Tasks - <code>biomlbench.data</code>","text":"<p>Task preparation, data downloading, and dataset management functionality.</p>"},{"location":"api/overview/#agents-biomlbenchagents","title":"Agents - <code>biomlbench.agents</code>","text":"<p>Agent execution system for running AI agents in containerized environments.</p>"},{"location":"api/overview/#grading-biomlbenchgrade","title":"Grading - <code>biomlbench.grade</code>","text":"<p>Evaluation and scoring system for agent submissions with biomedical metrics.</p>"},{"location":"api/overview/#data-sources-biomlbenchdata_sources","title":"Data Sources - <code>biomlbench.data_sources</code>","text":"<p>Pluggable data source system supporting Kaggle, Polaris, and custom sources.</p>"},{"location":"api/overview/#utilities-biomlbenchutils","title":"Utilities - <code>biomlbench.utils</code>","text":"<p>Shared utility functions for file operations, logging, and data processing.</p>"},{"location":"api/overview/#key-classes","title":"Key Classes","text":""},{"location":"api/overview/#task","title":"Task","text":"<p>Represents a biomedical benchmark task with metadata, data paths, and evaluation configuration.</p> <pre><code>from biomlbench import registry\n\ntask = registry.get_task(\"caco2-wang\")\nprint(f\"Task: {task.name}\")\nprint(f\"Domain: {task.domain}\")\nprint(f\"Type: {task.task_type}\")\n</code></pre>"},{"location":"api/overview/#registry","title":"Registry","text":"<p>Central registry for discovering and managing tasks across different biomedical domains.</p> <pre><code>from biomlbench.registry import registry\n\n# List all available tasks\ntasks = registry.list_task_ids()\n\n# Get tasks by domain (not implemented yet)\n# oncology_tasks = registry.get_tasks_by_domain(\"oncology\")\n\n# Get tasks by type (not implemented yet)\n# imaging_tasks = registry.get_tasks_by_type(\"medical_imaging\")\n</code></pre>"},{"location":"api/overview/#agent","title":"Agent","text":"<p>Represents an AI agent with execution configuration and container settings.</p> <pre><code>from agents.registry import registry as agent_registry\n\nagent = agent_registry.get_agent(\"dummy\")\nprint(f\"Agent: {agent.name}\")\nprint(f\"Privileged: {agent.privileged}\")\n</code></pre>"},{"location":"api/overview/#grader","title":"Grader","text":"<p>Handles task-specific evaluation metrics and scoring logic. </p> <p>Note: This works but we don't really use it anywhere yet. Possibly remove unless we really have some more complex metrics needed across tasks. (TODO)</p> <pre><code>from biomlbench.grade_helpers import Grader\n\ngrader = Grader(name=\"auc-roc\", grade_fn=\"biomlbench.metrics:average_precision_at_k\")\nscore = grader(submission_df, answers_df)\n</code></pre>"},{"location":"api/overview/#data-source-architecture","title":"Data Source Architecture","text":"<p>BioML-bench uses a pluggable data source system:</p> <pre><code>from biomlbench.data_sources import create_data_source, list_available_sources\nfrom pathlib import Path\n\n# List available data source types\navailable_sources = list_available_sources()\nprint(f\"Available sources: {available_sources}\")\n\n# Create Kaggle data source\nkaggle_source = create_data_source(\"kaggle\")\n\n# Download task data\nkaggle_source.download(\n    source_config={\"competition_id\": \"histopathologic-cancer-detection\"},\n    data_dir=Path(\"test/\")\n)\n\n# Get leaderboard\nleaderboard = kaggle_source.get_leaderboard(\n    source_config={\"competition_id\": \"histopathologic-cancer-detection\"}\n)\n</code></pre>"},{"location":"api/overview/#task-configuration","title":"Task Configuration","text":"<p>Tasks are configured via YAML files with biomedical-specific metadata:</p> <pre><code>id: caco2-wang\nname: \"Caco-2 Permeability Prediction\"\ntask_type: drug_discovery\ndomain: pharmacokinetics\ndifficulty: medium\n\ndataset:\n  answers: caco2-wang/prepared/private/answers.csv\n  sample_submission: caco2-wang/prepared/public/sample_submission.csv\n\ngrader:\n  name: rmse\n  grade_fn: biomlbench.tasks.caco2-wang.grade:grade\n\nbiomedical_metadata:\n  modality: \"molecular_properties\"\n  data_type: \"regression\"\n  clinical_relevance: \"drug_absorption\"\n</code></pre>"},{"location":"api/overview/#error-handling","title":"Error Handling","text":"<p>BioML-bench defines custom exceptions for different error conditions:</p> <pre><code>from biomlbench.data_sources.base import DataSourceError\nfrom biomlbench.grade_helpers import InvalidSubmissionError\n\ntry:\n    score = grader(submission, answers)\nexcept InvalidSubmissionError as e:\n    logger.warning(f\"Invalid submission: {e}\")\nexcept DataSourceError as e:\n    logger.error(f\"Data source error: {e}\")\n</code></pre>"},{"location":"api/overview/#logging","title":"Logging","text":"<p>BioML-bench uses structured logging throughout:</p> <pre><code>from biomlbench.utils import get_logger\n\nlogger = get_logger(__name__)\nlogger.info(\"Task preparation starting\")\nlogger.warning(\"Missing optional dependency\")\nlogger.error(\"Task preparation failed\")\n</code></pre>"},{"location":"api/overview/#extension-points","title":"Extension Points","text":""},{"location":"api/overview/#adding-data-sources","title":"Adding Data Sources","text":"<p>Implement the <code>DataSource</code> interface:</p> <pre><code>from biomlbench.data_sources.base import DataSource\n\nclass MyDataSource(DataSource):\n    def download(self, source_config, data_dir):\n        # Implementation\n        pass\n\n    def get_leaderboard(self, source_config):\n        # Implementation  \n        pass\n</code></pre> <p>See examples for kaggle: <code>biomlbench/data_sources/kaggle.py</code> and polaris: <code>biomlbench/data_sources/polaris.py</code>.</p>"},{"location":"api/overview/#adding-metrics","title":"Adding Metrics","text":"<p>Implement grading functions:</p> <pre><code>def my_metric(submission: pd.DataFrame, answers: pd.DataFrame) -&gt; float:\n    \"\"\"Custom biomedical metric.\"\"\"\n    # Implementation\n    return score\n</code></pre>"},{"location":"api/overview/#adding-tasks","title":"Adding Tasks","text":"<p>See the documentation on adding tasks.</p>"},{"location":"api/overview/#adding-agents","title":"Adding Agents","text":"<p>See the documentation on creating agents.</p>"},{"location":"api/overview/#type-hints","title":"Type Hints","text":"<p>BioML-bench uses type hints throughout for better IDE support:</p> <pre><code>from typing import List, Optional, Dict, Any\nfrom pathlib import Path\nimport pandas as pd\n\ndef process_task(\n    task_id: str,\n    data_dir: Optional[Path] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Process a biomedical task with type safety.\"\"\"\n    pass\n</code></pre>"},{"location":"api/registry/","title":"Registry API","text":"<p>Task discovery, loading, and organization system for biomedical benchmark tasks.</p>"},{"location":"api/registry/#biomlbench.registry.Task","title":"Task  <code>dataclass</code>","text":"<pre><code>Task(id, name, description, grader, answers, gold_submission, sample_submission, task_type, domain, difficulty, prepare_fn, raw_dir, private_dir, public_dir, checksums, leaderboard, biomedical_metadata=None, human_baselines=None, compute_requirements=None, data_source=None)\n</code></pre> <p>Represents a biomedical ML task in the BioML-bench framework.</p> <p>Extended from MLE-bench Competition class with biomedical-specific metadata.</p>"},{"location":"api/registry/#biomlbench.registry.Registry","title":"Registry","text":"<pre><code>Registry(data_dir=DEFAULT_DATA_DIR)\n</code></pre>"},{"location":"api/registry/#biomlbench.registry.Registry.get_task","title":"get_task","text":"<pre><code>get_task(task_id)\n</code></pre> <p>Fetch the task from the registry.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_tasks_dir","title":"get_tasks_dir","text":"<pre><code>get_tasks_dir()\n</code></pre> <p>Retrieves the tasks directory within the registry.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_splits_dir","title":"get_splits_dir","text":"<pre><code>get_splits_dir()\n</code></pre> <p>Retrieves the splits directory within the repository.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_tasks_by_domain","title":"get_tasks_by_domain","text":"<pre><code>get_tasks_by_domain(domain)\n</code></pre> <p>List all task IDs for a specific biomedical domain.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_tasks_by_type","title":"get_tasks_by_type","text":"<pre><code>get_tasks_by_type(task_type)\n</code></pre> <p>List all task IDs for a specific task type.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_lite_task_ids","title":"get_lite_task_ids","text":"<pre><code>get_lite_task_ids()\n</code></pre> <p>List all task IDs for the lite version (low difficulty tasks).</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_tasks_by_difficulty","title":"get_tasks_by_difficulty","text":"<pre><code>get_tasks_by_difficulty(difficulty)\n</code></pre> <p>List all task IDs for a specific difficulty level.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_data_dir","title":"get_data_dir","text":"<pre><code>get_data_dir()\n</code></pre> <p>Retrieves the data directory within the registry.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.set_data_dir","title":"set_data_dir","text":"<pre><code>set_data_dir(new_data_dir)\n</code></pre> <p>Sets the data directory within the registry.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.list_task_ids","title":"list_task_ids","text":"<pre><code>list_task_ids()\n</code></pre> <p>List all task IDs available in the registry, sorted alphabetically.</p>"},{"location":"api/registry/#usage-examples","title":"Usage Examples","text":""},{"location":"api/registry/#basic-task-access","title":"Basic Task Access","text":"<pre><code>from biomlbench.registry import registry\n\n# Get task information\ntask = registry.get_task(\"caco2-wang\")\nprint(f\"Task: {task.name} ({task.domain})\")\n\n# List all available tasks\nall_tasks = registry.list_task_ids()\nprint(f\"Available tasks: {len(all_tasks)}\")\n</code></pre>"},{"location":"api/registry/#task-configuration-format","title":"Task Configuration Format","text":"<p>Each task requires a <code>config.yaml</code> file:</p> <pre><code>id: caco2-wang\nname: \"Caco-2 Cell Permeability Prediction\"\ntask_type: drug_discovery\ndomain: admet\ndifficulty: medium\n\ndata_source:\n  type: polaris\n  benchmark_id: tdcommons/caco2-wang\n\ndataset:\n  answers: caco2-wang/prepared/private/answers.csv\n  sample_submission: caco2-wang/prepared/public/sample_submission.csv\n\ngrader:\n  name: mean-absolute-error\n  grade_fn: biomlbench.tasks.caco2-wang.grade:grade\n\npreparer: biomlbench.tasks.caco2-wang.prepare:prepare\n\nbiomedical_metadata:\n  modality: \"molecular\"\n  data_type: \"smiles_regression\"\n  clinical_relevance: \"drug_absorption\"\n\ncompute_requirements:\n  recommended_gpu_memory_gb: 4\n  estimated_runtime_minutes: 15\n  max_dataset_size_gb: 1\n</code></pre>"},{"location":"api/tasks/","title":"Tasks API","text":"<p>Task preparation, management, and data handling functionality.</p> <p>Data preparation and management for BioML-bench.</p> <p>This module provides high-level functions for downloading and preparing datasets from various sources (Kaggle, Polaris, etc.) using a pluggable data source architecture.</p> <p>BioML-bench tasks module.</p> <p>This module contains all biomedical task implementations for the BioML-bench framework. Tasks are organized by domain and include:</p> <ul> <li>Medical Imaging: histopathologic-cancer-detection, medical-segmentation, etc.</li> <li>Protein Engineering: proteingym-fitness, structure-prediction, etc.  </li> <li>Drug Discovery: molecular-property-prediction, admet-prediction, etc.</li> <li>Clinical Biomarkers: biomarker-discovery, clinical-prediction, etc.</li> </ul>"},{"location":"api/tasks/#biomlbench.data.create_prepared_dir","title":"create_prepared_dir","text":"<pre><code>create_prepared_dir(task)\n</code></pre> <p>Create public and private directories for a task.</p>"},{"location":"api/tasks/#biomlbench.data.download_and_prepare_dataset","title":"download_and_prepare_dataset","text":"<pre><code>download_and_prepare_dataset(task, keep_raw=True, overwrite_checksums=False, overwrite_leaderboard=False, skip_verification=False)\n</code></pre> <p>Download and prepare a dataset using the appropriate data source.</p> <p>Args:     task: Task to prepare     keep_raw: Whether to keep raw downloaded data     overwrite_checksums: Whether to overwrite existing checksums     overwrite_leaderboard: Whether to overwrite existing leaderboard     skip_verification: Whether to skip checksum verification</p>"},{"location":"api/tasks/#biomlbench.data.is_dataset_prepared","title":"is_dataset_prepared","text":"<pre><code>is_dataset_prepared(task, grading_only=False)\n</code></pre> <p>Checks if the task has non-empty <code>public</code> and <code>private</code> directories with the expected files.</p>"},{"location":"api/tasks/#biomlbench.data.ensure_leaderboard_exists","title":"ensure_leaderboard_exists","text":"<pre><code>ensure_leaderboard_exists(task, force=False)\n</code></pre> <p>Ensures the leaderboard for a given task exists.</p> <p>Args:     task: Task to ensure leaderboard for     force: Whether to force download/update of leaderboard</p> <p>Returns:     Path to the leaderboard file</p> <p>Raises:     FileNotFoundError: If leaderboard cannot be found or created</p>"},{"location":"api/tasks/#biomlbench.data.is_valid_prepare_fn","title":"is_valid_prepare_fn","text":"<pre><code>is_valid_prepare_fn(preparer_fn)\n</code></pre> <p>Checks if the <code>preparer_fn</code> takes three arguments: <code>raw</code>, <code>public</code> and <code>private</code>, in that order.</p>"},{"location":"api/tasks/#biomlbench.data.generate_checksums","title":"generate_checksums","text":"<pre><code>generate_checksums(target_dir, exts=None, exclude=None)\n</code></pre> <p>Generate checksums for the files directly under the target directory with the specified extensions.</p> <p>Args:     target_dir: directory to generate checksums for.     exts: List of file extensions to generate checksums for.     exclude: List of file paths to exclude from checksum generation.</p> <p>Returns:     A dictionary of form file: checksum.</p>"},{"location":"api/tasks/#biomlbench.data.get_last_modified","title":"get_last_modified","text":"<pre><code>get_last_modified(fpath)\n</code></pre> <p>Return the last modified time of a file.</p>"},{"location":"api/tasks/#biomlbench.data.file_cache","title":"file_cache","text":"<pre><code>file_cache(fn)\n</code></pre> <p>A decorator that caches results of a function with a Path argument, invalidating the cache when the file is modified.</p>"},{"location":"api/tasks/#biomlbench.data.get_checksum","title":"get_checksum","text":"<pre><code>get_checksum(fpath)\n</code></pre> <p>Compute MD5 checksum of a file.</p>"},{"location":"api/tasks/#biomlbench.data.get_leaderboard","title":"get_leaderboard","text":"<pre><code>get_leaderboard(task)\n</code></pre> <p>Load leaderboard data for a task.</p>"},{"location":"api/tasks/#biomlbench.data.prepare_human_baselines","title":"prepare_human_baselines","text":"<pre><code>prepare_human_baselines(task, force=False)\n</code></pre> <p>Prepare human baseline data for a task.</p> <p>Args:     task: Task to prepare human baselines for     force: Whether to force re-download of human baselines</p> <p>Returns:     Path to human baselines CSV file, or None if not available</p>"},{"location":"api/utils/","title":"Utilities API","text":"<p>Core utility functions for BioML-bench.</p>"},{"location":"api/utils/#biomlbench.utils.authenticate_kaggle_api","title":"authenticate_kaggle_api","text":"<pre><code>authenticate_kaggle_api()\n</code></pre> <p>Authenticates the Kaggle API and returns an authenticated API object, or raises an error if authentication fails.</p>"},{"location":"api/utils/#biomlbench.utils.read_jsonl","title":"read_jsonl","text":"<pre><code>read_jsonl(file_path, skip_commented_out_lines=False)\n</code></pre> <p>Read a JSONL file and return a list of dictionaries of its content.</p> <p>Args:     file_path (str): Path to the JSONL file.     skip_commented_out_lines (bool): If True, skip commented out lines.</p> <p>Returns:     list[dict]: List of dictionaries parsed from the JSONL file.</p>"},{"location":"api/utils/#biomlbench.utils.get_runs_dir","title":"get_runs_dir","text":"<pre><code>get_runs_dir()\n</code></pre> <p>Returns an absolute path to the directory storing runs.</p>"},{"location":"api/utils/#biomlbench.utils.get_module_dir","title":"get_module_dir","text":"<pre><code>get_module_dir()\n</code></pre> <p>Returns an absolute path to the BioML-bench module.</p>"},{"location":"api/utils/#biomlbench.utils.get_repo_dir","title":"get_repo_dir","text":"<pre><code>get_repo_dir()\n</code></pre> <p>Returns an absolute path to the repository directory.</p>"},{"location":"api/utils/#biomlbench.utils.generate_run_id","title":"generate_run_id","text":"<pre><code>generate_run_id(task_id, agent_id, run_group=None)\n</code></pre> <p>Creates a unique run ID for a specific task and agent combo</p>"},{"location":"api/utils/#biomlbench.utils.create_run_dir","title":"create_run_dir","text":"<pre><code>create_run_dir(task_id=None, agent_id=None, run_group=None)\n</code></pre> <p>Creates a directory for the run.</p>"},{"location":"api/utils/#biomlbench.utils.is_compressed","title":"is_compressed","text":"<pre><code>is_compressed(fpath)\n</code></pre> <p>Checks if the file is compressed.</p>"},{"location":"api/utils/#biomlbench.utils.compress","title":"compress","text":"<pre><code>compress(src, compressed, exist_ok=False)\n</code></pre> <p>Compresses the contents of a source directory to a compressed file.</p>"},{"location":"api/utils/#biomlbench.utils.extract","title":"extract","text":"<pre><code>extract(compressed, dst, recursive=False, already_extracted=set())\n</code></pre> <p>Extracts the contents of a compressed file to a destination directory.</p>"},{"location":"api/utils/#biomlbench.utils.is_empty","title":"is_empty","text":"<pre><code>is_empty(dir)\n</code></pre> <p>Checks if the directory is empty.</p>"},{"location":"api/utils/#biomlbench.utils.load_yaml","title":"load_yaml","text":"<pre><code>load_yaml(fpath)\n</code></pre> <p>Loads a YAML file and returns its contents as a dictionary.</p>"},{"location":"api/utils/#biomlbench.utils.in_ci","title":"in_ci","text":"<pre><code>in_ci()\n</code></pre> <p>Checks if the code is running in GitHub CI.</p>"},{"location":"api/utils/#biomlbench.utils.import_fn","title":"import_fn","text":"<pre><code>import_fn(fn_import_string)\n</code></pre> <p>Imports a function from a module given a string in the format <code>potentially.nested.module_name:fn_name</code>.</p> <p>Basically equivalent to <code>from potentially.nested.module_name import fn_name</code>.</p>"},{"location":"api/utils/#biomlbench.utils.get_path_to_callable","title":"get_path_to_callable","text":"<pre><code>get_path_to_callable(callable)\n</code></pre> <p>Retrieves the file path of the module where the given callable is defined.</p> <p>Args:     callable (Callable): The callable for which the module path is required.</p> <p>Returns:     Path: The relative path to the module file from the current working directory.</p> <p>Raises:     AssertionError: If the module does not have a file path.</p>"},{"location":"api/utils/#biomlbench.utils.get_diff","title":"get_diff","text":"<pre><code>get_diff(d, other_d, fromfile='d', tofile='other_d')\n</code></pre> <p>Finds the differences between two nested dictionaries and returns a diff string.</p>"},{"location":"api/utils/#biomlbench.utils.read_csv","title":"read_csv","text":"<pre><code>read_csv(*args, **kwargs)\n</code></pre> <p>Reads a CSV file and returns a DataFrame with custom default kwargs.</p>"},{"location":"api/utils/#biomlbench.utils.get_timestamp","title":"get_timestamp","text":"<pre><code>get_timestamp()\n</code></pre> <p>Returns the current timestamp in the format <code>YYYY-MM-DDTHH-MM-SS-Z</code>.</p>"},{"location":"api/utils/#biomlbench.utils.generate_submission_from_metadata","title":"generate_submission_from_metadata","text":"<pre><code>generate_submission_from_metadata(metadata_path, output_path=None, rel_log_path=Path('logs/agent.log'), rel_code_path=Path('code/train.py'))\n</code></pre> <p>Generate a submission.jsonl file from agent run metadata.</p> <p>This function reads the metadata.json file created by run_agent_async() and creates a JSONL file mapping task IDs to their submission file paths, which can be used directly with the <code>biomlbench grade</code> command.</p> <p>Args:     metadata_path: Path to the metadata.json file     output_path: Path for the output submission.jsonl file (defaults to same directory as metadata)     rel_log_path: Path to logfile relative to run directory     rel_code_path: Path to code file relative to run directory</p> <p>Returns:     Path to the generated submission.jsonl file</p>"},{"location":"components/agents/","title":"Agents Component","text":"<p>The agents system provides a framework for registering, building, and executing AI agents on biomedical tasks.</p>"},{"location":"components/agents/#available-agents","title":"Available Agents","text":"<p>BioML-bench includes several reference agents:</p>"},{"location":"components/agents/#dummy-agent","title":"Dummy Agent","text":"<p>A simple test agent for environment validation.</p>"},{"location":"components/agents/#aide-agent","title":"AIDE Agent","text":"<p>Advanced AI agent for automated biomedical research.</p>"},{"location":"components/agents/#mlagentbench","title":"MLAgentBench","text":"<p>Research agent for machine learning tasks.</p>"},{"location":"components/agents/#openhands-opendevin","title":"OpenHands (OpenDevin)","text":"<p>Code-writing agent with container management.</p>"},{"location":"components/agents/#agent-development","title":"Agent Development","text":"<p>See the adding agents docs for:</p> <ul> <li>Agent architecture</li> <li>Building custom agents</li> <li>Container requirements</li> </ul>"},{"location":"components/environment/","title":"Environment Reference","text":"<p>The <code>environment/</code> directory contains the containerized execution environment for BioML-bench agents, including the base Docker image, grading server, and configuration files.</p>"},{"location":"components/environment/#overview","title":"Overview","text":"<p>BioML-bench uses Docker containers to provide secure, isolated execution environments for AI agents. The environment includes:</p> <ul> <li>Base Docker image with biomedical libraries</li> <li>Grading server for submission validation</li> <li>Container configuration system</li> </ul>"},{"location":"components/environment/#core-components","title":"Core Components","text":""},{"location":"components/environment/#base-docker-image-dockerfile","title":"Base Docker Image (<code>Dockerfile</code>)","text":"<p>The foundational Docker image provides a complete biomedical ML environment:</p> <p>Base System:</p> <ul> <li>Ubuntu 22.04 LTS</li> <li>Python 3.11 via Miniconda</li> <li>Essential system packages and development tools</li> </ul> <p>Libraries:</p> <ul> <li>RDKit - Molecular informatics toolkit</li> <li>BioPython - Biological computation library</li> <li>scikit-learn - Machine learning framework</li> <li>pandas, numpy - Data processing</li> <li>TensorFlow with CUDA support</li> <li>PyTorch with GPU acceleration</li> </ul>"},{"location":"components/environment/#grading-server-grading_serverpy","title":"Grading Server (<code>grading_server.py</code>)","text":"<p>A Flask-based validation server that runs inside containers to evaluate agent submissions:</p> <p>Key Features:</p> <ul> <li>HTTP health check endpoint (<code>/health</code>)</li> <li>Submission validation against private answers</li> <li>Real-time evaluation feedback</li> <li>Resource monitoring and limits</li> </ul> <p>Usage in Containers: <pre><code># Server starts automatically via entrypoint.sh\n# Agents can check health status\ncurl http://localhost:5000/health\n\n# Server validates submissions internally\n# No direct agent interaction required\n</code></pre></p>"},{"location":"components/environment/#container-entrypoint-entrypointsh","title":"Container Entrypoint (<code>entrypoint.sh</code>)","text":"<p>The entry script that configures the container environment and starts services:</p> <p>Responsibilities:</p> <ul> <li>User environment setup (non-root execution)</li> <li>Directory permissions configuration</li> <li>Grading server initialization</li> <li>Agent execution orchestration</li> </ul> <p>Execution Flow:</p> <ol> <li>Validate container environment</li> <li>Set up user permissions and directories</li> <li>Start grading server in background</li> <li>Wait for server health check</li> <li>Execute agent start script</li> <li>Clean up resources on exit</li> </ol> <p>Security Features:</p> <ul> <li>Non-root user execution (<code>nonroot</code>)</li> <li>Private directory isolation (<code>/private/</code>)</li> <li>Read-only data mounts</li> <li>Resource limit enforcement</li> </ul>"},{"location":"components/environment/#task-instructions","title":"Task Instructions","text":"<p>Standardized instructions provided to agents. Includes info about general task structure, data, validation, and submission details.</p> <p><code>instructions.txt</code> - Complete task instructions.</p> <p><code>instructions_obfuscated.txt</code>  (TODO: NEEDS TO BE CHECKED) - Minimal instructions to prevent overfitting and data leakage.</p>"},{"location":"components/environment/#validation-script-validate_submissionsh","title":"Validation Script (<code>validate_submission.sh</code>)","text":"<p>Shell script for basic submission format validation. Agents are instructed to run this script to validate their submissions before finishing.</p>"},{"location":"components/environment/#container-configuration","title":"Container Configuration","text":""},{"location":"components/environment/#default-configuration-configcontainer_configsdefaultjson","title":"Default Configuration (<code>config/container_configs/default.json</code>)","text":"<pre><code>{\n    \"mem_limit\": null,       # No memory limit (use system default)\n    \"shm_size\": \"4G\",        # Shared memory for large datasets\n    \"nano_cpus\": 4e9         # 4 CPU cores\n}\n</code></pre>"},{"location":"components/environment/#custom-configuration-options","title":"Custom Configuration Options","text":"<p>Resource Limits: <pre><code>{\n    \"mem_limit\": \"8g\",          # 8GB memory limit\n    \"shm_size\": \"4g\",           # 4GB shared memory\n    \"nano_cpus\": 8e9,           # 8 CPU cores\n    \"gpus\": -1,                 # All available GPUs\n    \"runtime\": \"sysbox-runc\"    # Enhanced security runtime\n}\n</code></pre></p> <p>Security Settings: <pre><code>{\n    \"privileged\": false,        # Disable privileged mode\n    \"user\": \"nonroot\",          # Non-root user execution\n    \"read_only\": false,         # Allow container writes\n    \"security_opt\": [           # Security options\n        \"no-new-privileges:true\"\n    ]\n}\n</code></pre></p>"},{"location":"components/environment/#environment-variables","title":"Environment Variables","text":""},{"location":"components/environment/#system-environment","title":"System Environment","text":"<p>Set automatically by the container:</p> <pre><code># Directory paths\nDATA_DIR=\"/home/data\"\nSUBMISSION_DIR=\"/home/submission\"\nLOGS_DIR=\"/home/logs\"\nCODE_DIR=\"/home/code\"\nAGENT_DIR=\"/home/agent\"\n</code></pre>"},{"location":"components/environment/#volume-mounting-strategy","title":"Volume Mounting Strategy","text":""},{"location":"components/environment/#data-volume-structure","title":"Data Volume Structure","text":"<pre><code>/home/data/                    # Task data (read-only)\n\u251c\u2500\u2500 description.md             # Task description\n\u251c\u2500\u2500 train.csv                  # Training data\n\u251c\u2500\u2500 test_features.csv          # Test features\n\u251c\u2500\u2500 sample_submission.csv      # Expected format\n\u2514\u2500\u2500 human_baselines.csv        # Human performance (if available)\n\n/home/submission/              # Agent output (read-write)\n\u2514\u2500\u2500 submission.csv             # Agent predictions\n\n/private/data/task-id/         # Private evaluation data\n\u2514\u2500\u2500 prepared/private/\n    \u2514\u2500\u2500 answers.csv            # Ground truth (inaccessible to agents)\n</code></pre>"},{"location":"components/environment/#build-process","title":"Build Process","text":"<p>Size Optimization: - Multi-stage builds - Cleanup of package managers - Removal of development headers</p> <p>Example Build Command (NEEDS TO BE CHECKED): <pre><code>docker build \\\n    --target final \\\n    --build-arg INSTALL_HEAVY_DEPENDENCIES=true \\\n    --tag biomlbench-env \\\n    -f environment/Dockerfile \\\n    .\n</code></pre></p>"},{"location":"components/scripts/","title":"Scripts Reference","text":"<p>The <code>scripts/</code> directory contains automation scripts for building, testing, and managing BioML-bench environments and agents.</p>"},{"location":"components/scripts/#build-scripts","title":"Build Scripts","text":""},{"location":"components/scripts/#build_base_envsh","title":"<code>build_base_env.sh</code>","text":"<p>Builds the foundational Docker image (<code>biomlbench-env</code>) with biomedical libraries and dependencies.</p> <p>Usage: <pre><code>./scripts/build_base_env.sh\n</code></pre></p> <p>Features: - Installs Python 3.11 with conda - Includes biomedical libraries (RDKit, BioPython) - Configures ML frameworks (TensorFlow, PyTorch) - Sets up the grading server environment - Performs post-build validation tests</p> <p>Requirements: - Docker installed and running - Internet connection for downloading dependencies - ~10GB disk space for the image</p> <p>Example Output: <pre><code>\ud83e\uddec Building BioML-bench Base Environment\n=======================================\n\ud83d\udccb Pre-build checks...\n\u2705 environment/Dockerfile\n\u2705 environment/requirements.txt\n\ud83d\udd28 Building biomlbench-env Docker image...\n\u2705 Successfully built biomlbench-env image\n\ud83e\uddea Testing base image...\n\u2705 Python is available\n\u2705 BioML-bench is importable\n\u2705 Biomedical and ML dependencies are available\n\u2705 Agent conda environment is ready\n\ud83c\udf89 Base environment build completed successfully!\n</code></pre></p>"},{"location":"components/scripts/#build_agentsh","title":"<code>build_agent.sh</code>","text":"<p>Builds Docker images for individual agents.</p> <p>Usage: <pre><code>./scripts/build_agent.sh &lt;agent-name&gt;\n\n# Examples\n./scripts/build_agent.sh dummy\n./scripts/build_agent.sh aide\n</code></pre></p> <p>Features: - Builds agent-specific Docker images - Inherits from <code>biomlbench-env</code> base image - Configures agent-specific dependencies - Sets up environment variables and entrypoints</p> <p>Agent Directory Structure: <pre><code>agents/&lt;agent-name&gt;/\n\u251c\u2500\u2500 Dockerfile          # Agent-specific build instructions\n\u251c\u2500\u2500 config.yaml         # Agent configuration\n\u251c\u2500\u2500 start.sh            # Agent execution script\n\u251c\u2500\u2500 requirements.txt    # Additional dependencies\n\u2514\u2500\u2500 src/                # Agent source code\n</code></pre></p> <p>Additional Features: - Multi-stage builds for smaller images - Build caching optimization - Security scanning integration - Performance benchmarking - Automated testing of built images</p>"},{"location":"components/scripts/#testing-scripts","title":"Testing Scripts","text":""},{"location":"components/scripts/#test_environmentsh","title":"<code>test_environment.sh</code>","text":"<p>Comprehensive testing of the BioML-bench environment setup.</p> <p>Usage: <pre><code>./scripts/test_environment.sh\n</code></pre></p>"},{"location":"components/task_development/","title":"Task Development","text":"<p>Guide for creating new biomedical benchmark tasks in BioML-bench.</p>"},{"location":"components/task_development/#task-structure","title":"Task Structure","text":"<p>Each task requires:</p> <ul> <li><code>config.yaml</code> - Task configuration and metadata</li> <li><code>prepare.py</code> - Data preparation logic</li> <li><code>grade.py</code> - Evaluation function</li> <li><code>description.md</code> - Task description for agents</li> </ul>"},{"location":"components/task_development/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Create task directory:    <pre><code>mkdir -p biomlbench/tasks/my-new-task\n</code></pre></p> </li> <li> <p>Add configuration file</p> </li> <li>Implement preparation logic</li> <li>Define evaluation metrics</li> <li>Test with dummy agent</li> </ol> <p>See Adding Tasks Guide for detailed instructions. </p>"},{"location":"developer/adding_tasks/","title":"Adding Tasks","text":"<p>Guide for adding new biomedical benchmark tasks to BioML-bench.</p>"},{"location":"developer/adding_tasks/#task-requirements","title":"Task Requirements","text":"<p>Every task needs:</p> <ul> <li>Pre-split data for training and testing</li> <li>Clear evaluation metrics</li> <li>A description of the task</li> </ul> <p>Note: If adding a benchmarks from a new database, you'll also need to add a new data source module in <code>biomlbench/data_sources/</code>. See examples in <code>biomlbench/data_sources/kaggle.py</code> and <code>biomlbench/data_sources/polaris.py</code>.</p>"},{"location":"developer/adding_tasks/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Create task directory structure</li> <li>Configure task metadata</li> <li>Implement data preparation</li> <li>Define evaluation logic</li> <li>Write task description</li> <li>Test and validate</li> </ol>"},{"location":"developer/adding_tasks/#structure-of-task-directory","title":"Structure of task directory","text":"<pre><code>tasks/\n\u251c\u2500\u2500 my-biomedical-task/           # Task directory\n\u2502   \u251c\u2500\u2500 config.yaml              # Task configuration\n\u2502   \u251c\u2500\u2500 description.md           # Task description\n\u2502   \u251c\u2500\u2500 prepare.py               # Data preparation script\n\u2502   \u251c\u2500\u2500 grade.py                 # Evaluation logic\n\u2502   \u2514\u2500\u2500 prepared/                # Generated during preparation\n\u2502       \u251c\u2500\u2500 public/              # Public data\n\u2502       \u2502   \u251c\u2500\u2500 train.csv       # Training data\n\u2502       \u2502   \u251c\u2500\u2500 test_features.csv # Test features\n\u2502       \u2502   \u2514\u2500\u2500 sample_submission.csv # Example submission\n\u2502       \u2514\u2500\u2500 private/             # Private data\n\u2502           \u2514\u2500\u2500 answers.csv      # Test set answers\n</code></pre>"},{"location":"developer/adding_tasks/#task-configuration-configyaml","title":"Task Configuration (<code>config.yaml</code>)","text":"<pre><code>id: my-biomedical-task\nname: \"My Biomedical Task\"\ntask_type: drug_discovery  # or medical_imaging, protein_engineering\ndomain: pharmacokinetics   # specific biomedical domain\ndifficulty: medium         # easy, medium, hard\n\ndata_source:\n  type: kaggle            # or polaris, custom\n  competition_id: my-task\n\ndataset:\n  answers: my-task/prepared/private/answers.csv\n  sample_submission: my-task/prepared/public/sample_submission.csv\n\ngrader:\n  name: rmse\n  grade_fn: biomlbench.tasks.my-task.grade:grade\n\npreparer: biomlbench.tasks.my-task.prepare:prepare\n\nbiomedical_metadata:\n  modality: \"molecular_properties\"\n  organ_system: \"liver\"\n  data_type: \"regression\"\n  clinical_relevance: \"drug_metabolism\"\n</code></pre>"},{"location":"developer/adding_tasks/#data-preparation-preparepy","title":"Data Preparation (<code>prepare.py</code>)","text":"<p>See example <code>biomlbench/tasks/caco2-wang/prepare.py</code>.</p> <pre><code>from pathlib import Path\nimport pandas as pd\n\ndef prepare(task_dir: Path, raw_dir: Path, public_dir: Path, private_dir: Path) -&gt; Path:\n    \"\"\"Prepare task data with train/test splits.\"\"\"\n\n    # Download and process raw data\n    # Create train.csv, test_features.csv, sample_submission.csv\n    # Generate private answers.csv\n\n    return public_dir\n</code></pre>"},{"location":"developer/adding_tasks/#evaluation-logic-gradepy","title":"Evaluation Logic (<code>grade.py</code>)","text":"<p>See example <code>biomlbench/tasks/caco2-wang/grade.py</code>.</p> <pre><code>import pandas as pd\nimport numpy as np\n\ndef grade(submission: pd.DataFrame, answers: pd.DataFrame) -&gt; float:\n    \"\"\"Calculate task-specific metric.\"\"\"\n\n    y_true = answers['label'].values\n    y_pred = submission['label'].values\n\n    # Implement domain-specific metric\n    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n</code></pre>"},{"location":"developer/adding_tasks/#testing-new-tasks","title":"Testing New Tasks","text":"<pre><code># Test preparation\nbiomlbench prepare -t my-task\n\n# Test with dummy agent\nbiomlbench run-agent --agent dummy --task-id my-task\n\n# Validate submission\nbiomlbench grade --submission /path/to/submission.jsonl --output-dir /path/to/output/dir\n</code></pre>"},{"location":"developer/architecture/","title":"Architecture Overview","text":"<p>BioML-bench follows a modular architecture designed for extensibility and security.</p>"},{"location":"developer/architecture/#core-components","title":"Core Components","text":""},{"location":"developer/architecture/#registry-system","title":"Registry System","text":"<ul> <li>Tasks - Discovery and management of biomedical benchmark tasks</li> <li>Agents - Registration and configuration of AI agents</li> <li>Data Sources - Pluggable data acquisition from multiple repositories</li> </ul>"},{"location":"developer/architecture/#execution-engine","title":"Execution Engine","text":"<ul> <li>Containerization - Docker-based secure execution environment</li> <li>Parallel Processing - Multi-worker task execution</li> <li>Resource Management - Memory, CPU, and GPU allocation</li> </ul>"},{"location":"developer/architecture/#evaluation-framework","title":"Evaluation Framework","text":"<ul> <li>Domain-Specific Metrics - Biomedical evaluation functions</li> <li>Human Baselines - Expert performance comparisons</li> <li>Medal System - Kaggle-style performance ranking</li> </ul>"},{"location":"developer/architecture/#security-model","title":"Security Model","text":""},{"location":"developer/architecture/#container-isolation","title":"Container Isolation","text":"<ul> <li>Network isolation - No external network access during execution</li> <li>Filesystem isolation - Read-only data access, private answer protection</li> <li>User isolation - Non-root execution, limited capabilities</li> </ul>"},{"location":"developer/architecture/#data-protection","title":"Data Protection","text":"<ul> <li>Private/public splits - Clear separation of training and evaluation data</li> <li>Checksum validation - Data integrity verification</li> <li>Access controls - Restricted access to ground truth labels</li> </ul>"},{"location":"developer/architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Task Preparation - Download and process datasets</li> <li>Agent Execution - Run agents in isolated containers</li> <li>Submission Collection - Extract predictions and logs</li> <li>Evaluation - Score submissions against private answers</li> <li>Reporting - Generate comprehensive evaluation reports</li> </ol>"},{"location":"developer/architecture/#extension-points","title":"Extension Points","text":""},{"location":"developer/architecture/#adding-new-components","title":"Adding New Components","text":"<ul> <li>Tasks - Implement prepare/grade functions</li> <li>Agents - Create Docker containers with standard interface</li> <li>Data Sources - Implement DataSource interface</li> <li>Metrics - Define custom evaluation functions</li> </ul>"},{"location":"developer/architecture/#configuration-system","title":"Configuration System","text":"<ul> <li>YAML-based - Human-readable configuration files</li> <li>Environment variables - Runtime configuration options</li> <li>Container configs - Docker resource and security settings </li> </ul>"},{"location":"developer/contributing/","title":"Contributing to BioML-bench","text":"<p>We welcome contributions to BioML-bench! This guide covers how to contribute effectively.</p>"},{"location":"developer/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork and clone the repository</li> <li>Install development dependencies: <pre><code>uv sync --extra dev\n</code></pre></li> <li>Build the base environment: <pre><code>./scripts/build_base_env.sh\n</code></pre></li> </ol>"},{"location":"developer/contributing/#types-of-contributions","title":"Types of Contributions","text":"<ul> <li>New biomedical tasks - Add tasks from your domain expertise</li> <li>Agent implementations - Contribute new AI agents</li> <li>Data source integrations - Support new biomedical benchmark databases</li> <li>Documentation improvements - Help improve this documentation</li> <li>Bug fixes and optimizations - Code quality improvements</li> </ul>"},{"location":"developer/contributing/#contribution-process","title":"Contribution Process","text":"<ol> <li>Create a feature branch</li> <li>Make your changes</li> <li>Test thoroughly with existing tasks</li> <li>Submit a pull request</li> <li>Address review feedback</li> </ol>"},{"location":"developer/contributing/#code-standards","title":"Code Standards","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Add comprehensive docstrings</li> <li>Include type hints</li> <li>Update documentation for user-facing changes</li> </ul>"},{"location":"developer/contributing/#documentation","title":"Documentation","text":""},{"location":"developer/contributing/#building-documentation","title":"Building Documentation","text":"<p>BioML-bench uses MkDocs with auto-generated API documentation. When contributing:</p> <p>Build and serve docs locally: <pre><code># Build static docs\n./scripts/build_docs.sh\n\n# Build and serve docs locally\n./scripts/serve_docs.sh\n</code></pre></p> <p>Documentation is automatically generated from:</p> <ul> <li>Docstrings - All API documentation comes from code docstrings</li> <li>CLI help - CLI documentation is extracted from argparse definitions</li> <li>Manual content - User guides, tutorials, and conceptual documentation</li> </ul> <p>When to update documentation:</p> <ul> <li>Add docstrings to new functions/classes</li> <li>Update docstrings when changing function signatures</li> <li>Add examples to docstrings for complex functionality</li> <li>Manual docs are rarely needed for API changes</li> </ul>"},{"location":"developer/contributing/#testing","title":"Testing","text":"<p>Test your changes with: <pre><code># Run basic tests\n./scripts/test_environment.sh\n\n# Test specific agent\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# Validate new tasks\nbiomlbench prepare -t my-new-task\n</code></pre></p>"},{"location":"developer/creating_agents/","title":"Creating Agents","text":"<p>Guide for developing custom AI agents for BioML-bench.</p>"},{"location":"developer/creating_agents/#agent-requirements","title":"Agent Requirements","text":"<p>LLM agents must:</p> <ul> <li>Run autonomously in Docker containers without human intervention</li> <li>Read and comprehend biomedical task descriptions from <code>/home/data/description.md</code></li> <li>Analyze data and design appropriate ML approaches</li> <li>Implement and execute complete ML pipelines (preprocessing, training, evaluation)</li> <li>Generate predictions and write them to <code>/home/submission/submission.csv</code></li> <li>Handle domain-specific data (SMILES for molecules, medical images, protein sequences, etc.)</li> </ul> <p>Unlike traditional ML models, agents must solve tasks from scratch using only the task description and training data.</p>"},{"location":"developer/creating_agents/#agent-structure","title":"Agent Structure","text":"<pre><code>agents/my-agent/\n\u251c\u2500\u2500 Dockerfile           # Container build instructions\n\u251c\u2500\u2500 config.yaml         # Agent configuration\n\u251c\u2500\u2500 start.sh            # Execution entry point\n\u251c\u2500\u2500 requirements.txt    # Python dependencies\n\u2514\u2500\u2500 src/                # Agent source code (if needed)\n    \u2514\u2500\u2500 agent.py\n</code></pre>"},{"location":"developer/creating_agents/#configuration-configyaml","title":"Configuration (<code>config.yaml</code>)","text":"<p>See example <code>agents/aide/config.yaml</code>.</p> <pre><code>my-agent:\n  start: my-agent/start.sh\n  dockerfile: my-agent/Dockerfile\n  env_vars:\n    OPENAI_API_KEY: \"${OPENAI_API_KEY}\"\n  privileged: false  # Set true only if necessary\n</code></pre>"},{"location":"developer/creating_agents/#docker-setup-dockerfile","title":"Docker Setup (<code>Dockerfile</code>)","text":"<p>See example <code>agents/aide/Dockerfile</code>.</p> <pre><code>FROM biomlbench-env\n\n# Copy agent code\nCOPY src/ /home/agent/src/\nCOPY requirements.txt /home/agent/\n\n# Install dependencies\nWORKDIR /home/agent\nRUN /opt/conda/bin/conda run -n agent pip install -r requirements.txt\n\n# Set working directory\nWORKDIR /home\n</code></pre>"},{"location":"developer/creating_agents/#agent-interface","title":"Agent Interface","text":"<p>Agents are LLM-based systems that autonomously solve biomedical ML tasks end-to-end. They must:</p> <ol> <li>Understand the task by reading the description and analyzing the data</li> <li>Design an ML approach appropriate for the biomedical problem  </li> <li>Implement, train, and evaluate models</li> <li>Generate predictions for submission</li> </ol>"},{"location":"developer/creating_agents/#example-agent-structure","title":"Example Agent Structure","text":"<pre><code># src/agent.py\nimport pandas as pd\nfrom pathlib import Path\n\ndef main():\n    # 1. Read and understand the task\n    with open('/home/data/description.md', 'r') as f:\n        task_description = f.read()\n        # Agent parses task type, evaluation metric, clinical context\n\n    # 2. Analyze the data\n    train_df = pd.read_csv('/home/data/train.csv')\n    test_df = pd.read_csv('/home/data/test_features.csv')\n    sample_submission = pd.read_csv('/home/data/sample_submission.csv')\n\n    # 3. ... Agent Logic ...\n\n    # 4. Create submission in required format\n    submission = pd.DataFrame({\n        'id': test_df['id'],\n        'prediction': predictions  # Column name from sample_submission\n    })\n\n    submission.to_csv('/home/submission/submission.csv', index=False)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"developer/creating_agents/#agent-input-data","title":"Agent Input Data","text":"<p>All agent input data is mounted at <code>/home/data/</code> in the container:</p> <ul> <li><code>description.md</code>: Complete task description including:</li> <li>Biomedical background and clinical relevance</li> <li>Task type (regression, classification, segmentation) and evaluation metric</li> <li>Dataset information (sample counts, features, targets)</li> <li>Data format specifications and file descriptions</li> <li>Baseline approaches and molecular representation strategies</li> <li> <p>Scientific references and original benchmark links</p> </li> <li> <p><code>train.csv</code>: Training data with molecular features and targets</p> </li> <li><code>test_features.csv</code>: Test features with unique IDs (no target labels)</li> <li><code>sample_submission.csv</code>: Expected submission format with dummy predictions</li> <li><code>human_baselines.csv</code>: Human performance baselines (when available)</li> </ul>"},{"location":"developer/creating_agents/#building-and-testing","title":"Building and Testing","text":"<pre><code># Build agent\n./scripts/build_agent.sh my-agent # e.g., ./scripts/build_agent.sh aide\n\n# Test agent\nbiomlbench run-agent --agent my-agent --task-id caco2-wang # e.g., biomlbench run-agent --agent aide --task-id caco2-wang\n</code></pre>"}]}