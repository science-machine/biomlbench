{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BioML-bench","text":"<p>A benchmark suite for evaluating LLM agents on biomedical machine learning tasks.</p> <p>BioML-bench is built on top of MLE-bench and provides a comprehensive framework for benchmarking LLM agents on biomedical machine learning (BioML) tasks including protein engineering, drug discovery, medical imaging, and clinical biomarkers.</p> <p>Agents autonomously read task descriptions, analyze biomedical data, design appropriate ML approaches, and implement complete solutions from scratch.</p>"},{"location":"#features","title":"\ud83e\uddec Features","text":"<ul> <li>Diverse Biomedical Tasks: Protein engineering, drug discovery, medical imaging, clinical biomarkers</li> <li>Agent-Agnostic Evaluation: Any LLM agent that can read task descriptions and produce CSV submissions can be evaluated</li> <li>Human Baselines: Built-in human performance benchmarks for comparison</li> <li>Secure Evaluation: Containerized execution with no data leakage</li> <li>Extensible Framework: Easy to add new biomedical tasks</li> <li>Biomedical Libraries: Pre-installed RDKit, BioPython, and other domain-specific tools</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Install the package (requires uv):</p> <pre><code># Install with uv\ngit clone https://github.com/science-machine/biomlbench.git\ncd biomlbench\nuv sync\n</code></pre> <p>Build and run a benchmark with an agent:</p> <pre><code># Prepare a task\nbiomlbench prepare -t caco2-wang\n\n# Run an agent (in this case, a dummy agent that returns null predictions)\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# Grade results (submission.jsonl is auto-generated)\nbiomlbench grade --submission &lt;run-group-dir&gt;/submission.jsonl --output-dir results/\n</code></pre>"},{"location":"#available-tasks","title":"\ud83d\udcca Available Tasks","text":""},{"location":"#medical-imaging","title":"Medical Imaging","text":"<ul> <li>histopathologic-cancer-detection: Cancer detection in histopathology patches</li> </ul>"},{"location":"#drug-discovery","title":"Drug Discovery","text":"<ul> <li>caco2-wang: Molecular property prediction (intestinal permeability)</li> </ul>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>BioML-bench follows a modular architecture:</p> <ul> <li>Core Framework (<code>biomlbench/</code>) - Task management, grading, data handling</li> <li>Agent System (<code>agents/</code>) - Agent registry and execution framework  </li> <li>Environment (<code>environment/</code>) - Containerized execution environment</li> <li>Tasks (<code>biomlbench/tasks/</code>) - Individual biomedical benchmark tasks</li> <li>Scripts (<code>scripts/</code>) - Build, test, and deployment automation</li> </ul>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":"<ul> <li>Quick Start - Get up and running</li> <li>API Reference - Complete API documentation</li> <li>Components - Deep dive into system components</li> <li>Developer Guide - Extend the framework</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! See our Contributing Guide for details on:</p> <ul> <li>Adding new biomedical tasks</li> <li>Creating custom agents</li> <li>Extending data sources</li> <li>Improving documentation </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers installing BioML-bench and setting up the environment for running agents on biomedical tasks.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Docker - For containerized agent execution</li> <li>uv - Python package manager (recommended)</li> </ul>"},{"location":"installation/#install-uv","title":"Install uv","text":"<p>BioML-bench uses uv for fast dependency management:</p> <pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or with pip\npip install uv\n</code></pre>"},{"location":"installation/#install-docker","title":"Install Docker","text":"<p>Docker is required for secure agent execution:</p> Ubuntu/DebianmacOSWindows <pre><code>sudo apt update\nsudo apt install docker.io\nsudo systemctl start docker\nsudo usermod -aG docker $USER\n# Log out and back in for group changes\n</code></pre> <pre><code># Install Docker Desktop from https://docker.com/products/docker-desktop\n# Or with Homebrew\nbrew install --cask docker\n</code></pre> <p>Download and install Docker Desktop.</p>"},{"location":"installation/#installing-bioml-bench","title":"Installing BioML-bench","text":""},{"location":"installation/#from-source-recommended-for-development","title":"From Source (Recommended for Development)","text":"<pre><code># Clone the repository\ngit clone https://github.com/science-machine/biomlbench.git\ncd biomlbench\n\n# Install dependencies with uv\nuv sync\n\n# Activate the virtual environment\nsource .venv/bin/activate  # Linux/macOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Verify installation\nbiomlbench --help\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For contributing to BioML-bench:</p> <pre><code># Clone and install in development mode\ngit clone https://github.com/science-machine/biomlbench.git\ncd biomlbench\n\n# Install with development dependencies\nuv sync --extra dev\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"installation/#environment-setup","title":"Environment Setup","text":""},{"location":"installation/#build-base-docker-environment","title":"Build Base Docker Environment","text":"<p>Before running agents, build the base Docker environment with biomedical libraries:</p> <pre><code># Build the biomlbench-env image\n./scripts/build_base_env.sh\n\n# Verify the environment\n./scripts/test_environment.sh\n</code></pre>"},{"location":"installation/#optional-install-heavy-dependencies","title":"Optional: Install Heavy Dependencies","text":"<p>For tasks requiring additional biomedical libraries:</p> <pre><code># Install heavy dependencies (RDKit, BioPython, etc.)\nuv sync --extra heavy\n</code></pre>"},{"location":"installation/#gpu-support-optional","title":"GPU Support (Optional)","text":"<p>For GPU-accelerated tasks, install NVIDIA Container Toolkit:</p> <pre><code># Ubuntu/Debian\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n\nsudo apt update\nsudo apt install nvidia-container-toolkit\nsudo systemctl restart docker\n</code></pre>"},{"location":"installation/#configuration","title":"Configuration","text":""},{"location":"installation/#kaggle-api-for-kaggle-based-tasks","title":"Kaggle API (For Kaggle-based Tasks)","text":"<p>Some tasks require Kaggle data. Set up Kaggle API credentials:</p> <pre><code># Install Kaggle API\npip install kaggle\n\n# Download API credentials from https://www.kaggle.com/account\n# Place in ~/.kaggle/kaggle.json (Linux/macOS) or %USERPROFILE%\\.kaggle\\kaggle.json (Windows)\n\n# Set permissions (Linux/macOS only)\nchmod 600 ~/.kaggle/kaggle.json\n</code></pre>"},{"location":"installation/#agent-api-keys","title":"Agent API Keys","text":"<p>For agents that require API access (e.g., AIDE):</p> <pre><code># Create environment file\necho \"OPENAI_API_KEY=your-key-here\" &gt; .env\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code># Check CLI is working\nbiomlbench --help\n\n# List available tasks\nbiomlbench prepare --help\n\n# Test with dummy agent (requires Docker)\nbiomlbench prepare -t caco2-wang\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":"<p>Docker permission denied: <pre><code>sudo usermod -aG docker $USER\n# Log out and back in\n</code></pre></p> <p>uv not found: <pre><code># Restart shell or source your shell profile\nsource ~/.bashrc  # or ~/.zshrc\n</code></pre></p> <p>Agent build fails: <pre><code># Ensure base environment is built first\n./scripts/build_base_env.sh\n</code></pre></p> <p>Task preparation fails: <pre><code># Check Kaggle API credentials\nkaggle competitions list\n</code></pre></p>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<ul> <li>Check the FAQ</li> <li>Review Common Issues</li> <li>Open an issue on GitHub </li> </ul>"},{"location":"usage/","title":"Basic Usage","text":"<p>This guide covers the basic workflow for using BioML-bench to evaluate agents on biomedical tasks.</p>"},{"location":"usage/#overview","title":"Overview","text":"<p>The typical BioML-bench workflow involves:</p> <ol> <li>Prepare task datasets</li> <li>Run agents on tasks  </li> <li>Grade agent submissions</li> <li>Analyze results</li> </ol>"},{"location":"usage/#task-preparation","title":"Task Preparation","text":"<p>Before running agents, you need to prepare task datasets:</p>"},{"location":"usage/#prepare-individual-tasks","title":"Prepare Individual Tasks","text":"<pre><code># Prepare a specific task\nbiomlbench prepare -t caco2-wang\n\n# Prepare with options\nbiomlbench prepare -t histopathologic-cancer-detection --keep-raw\n</code></pre>"},{"location":"usage/#prepare-multiple-tasks","title":"Prepare Multiple Tasks","text":"<pre><code># Prepare by difficulty\nbiomlbench prepare --lite  # Easy/medium tasks only\n\n# Prepare by domain\nbiomlbench prepare --domain oncology\n\n# Prepare by task type  \nbiomlbench prepare --task-type medical_imaging\n\n# Prepare from a list\nbiomlbench prepare --list experiments/splits/caco2-wang.txt\n\n# Prepare all tasks\nbiomlbench prepare --all\n</code></pre>"},{"location":"usage/#available-task-filters","title":"Available Task Filters","text":"<ul> <li><code>--lite</code>: Low difficulty tasks for quick testing</li> <li><code>--domain</code>: Filter by biomedical domain (admet, oncology, etc.)</li> <li><code>--task-type</code>: Filter by task type (medical_imaging, drug_discovery, etc.)</li> <li><code>--list</code>: Use a custom task list file</li> </ul> <p>Note: Currently only <code>experiments/splits/caco2-wang.txt</code> exists. Additional split files for domains and task types are planned for future releases.</p>"},{"location":"usage/#running-agents","title":"Running Agents","text":""},{"location":"usage/#built-in-agents","title":"Built-in Agents","text":"<p>BioML-bench includes several reference agents:</p> <pre><code># Dummy agent (for testing)\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# AIDE agent (requires OpenAI API key)\nbiomlbench run-agent --agent aide --task-id caco2-wang\n</code></pre>"},{"location":"usage/#single-task-execution","title":"Single Task Execution","text":"<pre><code># Run agent on one task\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# With custom settings\nbiomlbench run-agent \\\n    --agent dummy \\\n    --task-id caco2-wang \\\n    --n-seeds 3 \\\n    --retain-container\n</code></pre>"},{"location":"usage/#multi-task-execution","title":"Multi-Task Execution","text":"<pre><code># Run on multiple tasks from a list (example with available split)\nbiomlbench run-agent --agent dummy --task-list experiments/splits/caco2-wang.txt\n\n# Parallel execution (future: when more split files are available)\n# biomlbench run-agent \\\n#     --agent dummy \\\n#     --task-list experiments/splits/medical-imaging.txt \\\n#     --n-workers 4 \\\n#     --n-seeds 2\n</code></pre>"},{"location":"usage/#agent-execution-options","title":"Agent Execution Options","text":"<ul> <li><code>--n-workers</code>: Number of parallel workers (default: 1)</li> <li><code>--n-seeds</code>: Random seeds per task (default: 1)  </li> <li><code>--retain-container</code>: Keep containers for debugging</li> <li><code>--container-config</code>: Custom Docker configuration</li> <li><code>--data-dir</code>: Custom data directory</li> </ul>"},{"location":"usage/#understanding-agent-outputs","title":"Understanding Agent Outputs","text":"<p>When an agent run completes, BioML-bench creates:</p> <pre><code>runs/\n\u2514\u2500\u2500 2024-01-15T10-30-00-GMT_run-group_dummy/\n    \u251c\u2500\u2500 metadata.json              # Run summary\n    \u251c\u2500\u2500 submission.jsonl           # Auto-generated submission file\n    \u251c\u2500\u2500 caco2-wang_abc123/         # Individual task run\n    \u2502   \u251c\u2500\u2500 submission/\n    \u2502   \u2502   \u2514\u2500\u2500 submission.csv     # Agent predictions\n    \u2502   \u251c\u2500\u2500 logs/\n    \u2502   \u2502   \u2514\u2500\u2500 run.log           # Execution logs\n    \u2502   \u2514\u2500\u2500 code/                 # Agent code (if available)\n    \u2514\u2500\u2500 task2_def456/             # Additional task runs...\n</code></pre>"},{"location":"usage/#key-files","title":"Key Files","text":"<ul> <li><code>metadata.json</code>: Run metadata and success status</li> <li><code>submission.jsonl</code>: Automatically generated submission index</li> <li><code>submission.csv</code>: Agent's predictions for each task</li> <li><code>run.log</code>: Detailed execution logs</li> </ul>"},{"location":"usage/#grading-and-evaluation","title":"Grading and Evaluation","text":""},{"location":"usage/#grade-agent-runs","title":"Grade Agent Runs","text":"<p>The submission file is automatically generated after agent execution:</p> <pre><code># Grade using auto-generated submission file\nbiomlbench grade \\\n    --submission runs/2024-01-15T10-30-00-GMT_run-group_dummy/submission.jsonl \\\n    --output-dir results/\n</code></pre>"},{"location":"usage/#grade-individual-submissions","title":"Grade Individual Submissions","text":"<p>For testing or external submissions:</p> <pre><code># Grade a single CSV file\nbiomlbench grade-sample submission.csv caco2-wang\n</code></pre>"},{"location":"usage/#baseline-comparisons","title":"Baseline Comparisons","text":"<p>Run baselines to establish performance benchmarks:</p> <pre><code># Run specific baseline\nbiomlbench run-baseline caco2-wang --baseline linear\n\n# Run all available baselines\nbiomlbench run-baseline caco2-wang --baseline all\n\n# Grade baseline results\nbiomlbench grade --submission baseline_submissions/submission.jsonl --output-dir results/\n</code></pre>"},{"location":"usage/#working-with-results","title":"Working with Results","text":""},{"location":"usage/#understanding-scores","title":"Understanding Scores","text":"<p>Each task uses task/domain-specific metrics.</p> <p>E.g., Caco2-Wang uses MAE, while Histopathologic Cancer Detection uses AUC-ROC.</p>"},{"location":"usage/#medal-system","title":"Medal System","text":"<p>BioML-bench uses a Kaggle-style medal system that varies based on leaderboard size:</p> <p>For small leaderboards (1-99 teams): - \ud83e\udd47 Gold: Top 10% of submissions - \ud83e\udd48 Silver: Top 20% (but not gold) - \ud83e\udd49 Bronze: Top 40% (but not silver/gold)</p> <p>For medium leaderboards (100-249 teams): - \ud83e\udd47 Gold: Top 10 positions (fixed) - \ud83e\udd48 Silver: Top 20% (but not gold) - \ud83e\udd49 Bronze: Top 40% (but not silver/gold)</p> <p>For large leaderboards (250-999 teams): - \ud83e\udd47 Gold: Top (10 + 0.2% of teams) positions - \ud83e\udd48 Silver: Top 50 positions (fixed) - \ud83e\udd49 Bronze: Top 100 positions (fixed)</p> <p>For very large leaderboards (1000+ teams): - \ud83e\udd47 Gold: Top (10 + 0.2% of teams) positions - \ud83e\udd48 Silver: Top 5% of submissions - \ud83e\udd49 Bronze: Top 10% of submissions</p> <p>This follows the official Kaggle competition progression system.</p>"},{"location":"usage/#human-baseline-comparison","title":"Human Baseline Comparison","text":"<p>Many tasks include human expert performance for context:</p> <pre><code>{\n  \"task_id\": \"histopathologic-cancer-detection\",\n  \"score\": 0.89,\n  \"beats_human\": true,\n  \"human_percentile\": 85.5\n}\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/#custom-container-configuration","title":"Custom Container Configuration","text":"<p>Override Docker settings for specific requirements:</p> <pre><code>// custom_config.json\n{\n    \"mem_limit\": \"8g\",\n    \"shm_size\": \"4g\", \n    \"nano_cpus\": 4000000000,\n    \"gpus\": -1\n}\n</code></pre> <pre><code>biomlbench run-agent \\\n    --agent my-agent \\\n    --task-id caco2-wang \\\n    --container-config custom_config.json\n</code></pre>"},{"location":"usage/#custom-data-directory","title":"Custom Data Directory","text":"<p>Use a different data storage location:</p> <pre><code>biomlbench prepare -t caco2-wang --data-dir /custom/data/path\n\nbiomlbench run-agent \\\n    --agent dummy \\\n    --task-id caco2-wang \\\n    --data-dir /custom/data/path\n</code></pre>"},{"location":"usage/#task-development-workflow","title":"Task Development Workflow","text":"<pre><code># 1. Create new task structure\nmkdir -p biomlbench/tasks/my-new-task\n\n# 2. Test task preparation\nbiomlbench prepare -t my-new-task\n\n# 3. Test with dummy agent\nbiomlbench run-agent --agent dummy --task-id my-new-task\n\n# 4. Grade test submission\nbiomlbench grade-sample runs/dummy/my-new-task/submission.csv my-new-task\n</code></pre>"},{"location":"usage/#cli-reference","title":"CLI Reference","text":""},{"location":"usage/#main-commands","title":"Main Commands","text":"<ul> <li><code>prepare</code>: Download and prepare task datasets</li> <li><code>run-agent</code>: Execute agents on tasks</li> <li><code>grade</code>: Evaluate submissions  </li> <li><code>grade-sample</code>: Test individual submissions</li> <li><code>run-baseline</code>: Generate baseline comparisons</li> </ul>"},{"location":"usage/#getting-help","title":"Getting Help","text":"<pre><code># Main help\nbiomlbench --help\n\n# Command-specific help\nbiomlbench prepare --help\nbiomlbench run-agent --help\nbiomlbench grade --help\n</code></pre>"},{"location":"usage/#common-workflows","title":"Common Workflows","text":""},{"location":"usage/#quick-testing","title":"Quick Testing","text":"<pre><code># Fast workflow for testing\nbiomlbench prepare -t caco2-wang\nbiomlbench run-agent --agent dummy --task-id caco2-wang\nbiomlbench grade --submission runs/*/submission.jsonl --output-dir results/\n</code></pre>"},{"location":"usage/#full-evaluation","title":"Full Evaluation","text":"<pre><code># Comprehensive evaluation (when more tasks are available)\n# biomlbench prepare --all\n# biomlbench run-agent --agent my-agent --task-list experiments/splits/all.txt --n-workers 4\n# biomlbench grade --submission runs/*/submission.jsonl --output-dir results/\n\n# Current example with available tasks\nbiomlbench prepare -t caco2-wang -t histopathologic-cancer-detection\nbiomlbench run-agent --agent my-agent --task-id caco2-wang\nbiomlbench grade --submission runs/*/submission.jsonl --output-dir results/\n</code></pre>"},{"location":"usage/#development-cycle","title":"Development Cycle","text":"<pre><code># Iterative development\nbiomlbench prepare -t my-task\nbiomlbench run-agent --agent my-agent --task-id my-task --retain-container\n# Debug containers, fix issues, repeat\n</code></pre>"},{"location":"api/agents/","title":"Agents API","text":"<p>The agents module provides functionality for running AI agents in containerized environments on biomedical tasks.</p>"},{"location":"api/agents/#core-functions","title":"Core Functions","text":"<p>Run an agent on multiple tasks asynchronously.</p> <p>Returns:     Tuple[str, Path]: The run group ID and path to the generated submission file</p> <p>Main entry point for running agents from the CLI.</p> <p>Args:     args: Parsed command line arguments</p> <p>Returns:     str: The run group ID for this execution</p>"},{"location":"api/agents/#data-classes","title":"Data Classes","text":"<p>Represents a single agent-task execution.</p>"},{"location":"api/agents/#worker-functions","title":"Worker Functions","text":"<p>Worker function that processes agent tasks from the queue.</p>"},{"location":"api/agents/#utility-functions","title":"Utility Functions","text":"<p>Get list of task IDs from either single task or task list file.</p> <p>Create a temporary task list file for single task execution.</p>"},{"location":"api/agents/#agent-execution-workflow","title":"Agent Execution Workflow","text":"<pre><code>import asyncio\nfrom biomlbench.agents import run_agent_async\n\n# Run agent asynchronously\nrun_group, submission_path = await run_agent_async(\n    agent_id=\"dummy\",\n    task_ids=[\"caco2-wang\"],\n    n_workers=1,\n    n_seeds=1\n)\n\nprint(f\"Run completed: {run_group}\")\nprint(f\"Submission ready: {submission_path}\")\n</code></pre>"},{"location":"api/agents/#container-management","title":"Container Management","text":"<p>The agents module handles Docker container lifecycle:</p> <ol> <li>Container Creation - Spins up task-specific containers</li> <li>Volume Mounting - Mounts task data and output directories</li> <li>Environment Setup - Configures environment variables</li> <li>Agent Execution - Runs agent start script</li> <li>Output Collection - Extracts submissions and logs</li> <li>Cleanup - Removes containers (unless retained)</li> </ol>"},{"location":"api/agents/#parallel-execution","title":"Parallel Execution","text":"<p>Support for running multiple tasks in parallel:</p> <pre><code># Run multiple tasks with parallel workers\nawait run_agent_async(\n    agent_id=\"aide\",\n    task_ids=[\"caco2-wang\", \"histopathologic-cancer-detection\"],\n    n_workers=4,  # 4 parallel workers\n    n_seeds=2     # 2 random seeds per task\n)\n</code></pre>"},{"location":"api/agents/#configuration","title":"Configuration","text":"<p>Agent execution can be customized via:</p> <ul> <li>Container config - Docker resource limits and settings</li> <li>Environment variables - Agent-specific configuration</li> <li>Data directory - Custom data storage location</li> <li>Retention policy - Keep containers for debugging </li> </ul>"},{"location":"api/cli/","title":"CLI Reference","text":"<p>BioML-bench's command-line interface provides comprehensive tools for managing biomedical benchmark tasks and running agent evaluations.</p>"},{"location":"api/cli/#main-entry-point","title":"Main Entry Point","text":""},{"location":"api/cli/#commands-overview","title":"Commands Overview","text":"<p>The CLI provides several subcommands for different operations:</p>"},{"location":"api/cli/#task-management","title":"Task Management","text":"<ul> <li><code>prepare</code> - Download and prepare task datasets</li> <li><code>dev download-leaderboard</code> - Download competition leaderboards</li> <li><code>dev prepare-human-baselines</code> - Extract human performance baselines</li> </ul>"},{"location":"api/cli/#agent-execution","title":"Agent Execution","text":"<ul> <li><code>run-agent</code> - Execute AI agents on biomedical tasks</li> <li><code>run-baseline</code> - Run baseline algorithms for comparison</li> </ul>"},{"location":"api/cli/#evaluation","title":"Evaluation","text":"<ul> <li><code>grade</code> - Evaluate agent submissions across multiple tasks</li> <li><code>grade-sample</code> - Test individual task submissions</li> </ul>"},{"location":"api/cli/#command-examples","title":"Command Examples","text":""},{"location":"api/cli/#prepare-tasks","title":"Prepare Tasks","text":"<pre><code># Single task\nbiomlbench prepare -t caco2-wang\n\n# Multiple tasks by domain\nbiomlbench prepare --domain oncology\n\n# All easy/medium tasks\nbiomlbench prepare --lite\n\n# Custom data directory\nbiomlbench prepare -t caco2-wang --data-dir /custom/path\n</code></pre>"},{"location":"api/cli/#run-agents","title":"Run Agents","text":"<pre><code># Single task\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# Multiple tasks with parallelization (example with available split)\nbiomlbench run-agent \\\n    --agent aide \\\n    --task-list experiments/splits/caco2-wang.txt \\\n    --n-workers 4 \\\n    --n-seeds 2\n\n# With custom container configuration\nbiomlbench run-agent \\\n    --agent my-agent \\\n    --task-id caco2-wang \\\n    --container-config custom_docker.json \\\n    --retain-container\n</code></pre>"},{"location":"api/cli/#grade-submissions","title":"Grade Submissions","text":"<pre><code># Grade agent run (submission.jsonl auto-generated)\nbiomlbench grade \\\n    --submission runs/2024-01-15_run-group_dummy/submission.jsonl \\\n    --output-dir results/\n\n# Test single submission\nbiomlbench grade-sample submission.csv caco2-wang\n</code></pre>"},{"location":"api/cli/#run-baselines","title":"Run Baselines","text":"<pre><code># Specific baseline\nbiomlbench run-baseline caco2-wang --baseline linear\n\n# All baselines\nbiomlbench run-baseline caco2-wang --baseline all\n\n# Custom output directory\nbiomlbench run-baseline caco2-wang --baseline rf --output-dir baselines/\n</code></pre>"},{"location":"api/cli/#command-line-arguments","title":"Command Line Arguments","text":""},{"location":"api/cli/#global-options","title":"Global Options","text":"<p>All commands support these options:</p> <ul> <li><code>--help</code> - Show command help</li> <li><code>--data-dir</code> - Custom data directory (overrides default cache location)</li> </ul>"},{"location":"api/cli/#task-filtering","title":"Task Filtering","text":"<p>For <code>prepare</code> command:</p> <ul> <li><code>-t, --task-id</code> - Specific task ID</li> <li><code>--all</code> - All available tasks  </li> <li><code>--lite</code> - Easy/medium difficulty tasks only</li> <li><code>--domain</code> - Tasks from specific biomedical domain</li> <li><code>--task-type</code> - Tasks of specific type (medical_imaging, drug_discovery, etc.)</li> <li><code>-l, --list</code> - Tasks from file (one per line)</li> </ul>"},{"location":"api/cli/#agent-execution_1","title":"Agent Execution","text":"<p>For <code>run-agent</code> command:</p> <ul> <li><code>--agent</code> - Agent ID (required)</li> <li><code>--task-id</code> - Single task to run</li> <li><code>--task-list</code> - File with multiple tasks</li> <li><code>--n-workers</code> - Parallel workers (default: 1)</li> <li><code>--n-seeds</code> - Random seeds per task (default: 1)</li> <li><code>--container-config</code> - Custom Docker configuration JSON</li> <li><code>--retain-container</code> - Keep containers after run</li> <li><code>--output-dir</code> - Custom output directory</li> </ul>"},{"location":"api/cli/#evaluation-options","title":"Evaluation Options","text":"<p>For <code>grade</code> command:</p> <ul> <li><code>--submission</code> - Path to submission JSONL file (required)</li> <li><code>--output-dir</code> - Results directory (required)</li> </ul> <p>For <code>grade-sample</code> command:</p> <ul> <li><code>submission</code> - Path to CSV file (positional)</li> <li><code>task_id</code> - Task ID (positional)</li> </ul>"},{"location":"api/cli/#environment-variables","title":"Environment Variables","text":"<p>BioML-bench respects these environment variables:</p>"},{"location":"api/cli/#agent-configuration","title":"Agent Configuration","text":"<ul> <li><code>OPENAI_API_KEY</code> - For AIDE agent</li> <li><code>I_ACCEPT_RUNNING_PRIVILEGED_CONTAINERS</code> - Allow privileged containers (set to \"true\")</li> </ul>"},{"location":"api/cli/#kaggle-api","title":"Kaggle API","text":"<p>Kaggle authentication uses the standard Kaggle API configuration file (<code>~/.kaggle/kaggle.json</code>). See the Kaggle API documentation for setup instructions.</p>"},{"location":"api/cli/#configuration-files","title":"Configuration Files","text":""},{"location":"api/cli/#container-configuration","title":"Container Configuration","text":"<p>Custom Docker settings (JSON format):</p> <pre><code>{\n    \"mem_limit\": \"8g\",\n    \"shm_size\": \"4g\",\n    \"nano_cpus\": 4000000000,\n    \"gpus\": -1,\n    \"runtime\": \"sysbox-runc\"\n}\n</code></pre>"},{"location":"api/cli/#task-lists","title":"Task Lists","text":"<p>Task list files contain one task ID per line:</p> <pre><code>caco2-wang\nhistopathologic-cancer-detection\n# Comments are supported\n</code></pre>"},{"location":"api/cli/#exit-codes","title":"Exit Codes","text":"<p>BioML-bench uses standard exit codes:</p> <ul> <li><code>0</code> - Success</li> <li><code>1</code> - General error</li> <li><code>2</code> - Invalid command line arguments</li> <li><code>130</code> - Interrupted by user (Ctrl+C)</li> </ul>"},{"location":"api/cli/#logging","title":"Logging","text":"<p>CLI output includes structured logging:</p> <pre><code>[2024-01-15 10:30:00,123] [cli.py:45] Running agent 'dummy' on tasks: ['caco2-wang']\n[2024-01-15 10:30:05,456] [agents.py:78] Launching run group: 2024-01-15T10-30-00-GMT_run-group_dummy\n[2024-01-15 10:32:15,789] [utils.py:234] Generated submission file: runs/.../submission.jsonl\n</code></pre> <p>Log levels can be controlled via Python logging configuration.</p>"},{"location":"api/cli/#shell-completion","title":"Shell Completion","text":"<p>Generate shell completion for improved CLI experience:</p> <pre><code># Bash\nbiomlbench --help | grep -A 20 \"completion\"\n\n# Zsh  \ncompinit &amp;&amp; complete -o default -F _biomlbench biomlbench\n\n# Fish\nbiomlbench --help | fish_completion\n</code></pre>"},{"location":"api/data_sources/","title":"Data Sources API","text":"<p>The data sources module provides a pluggable system for downloading datasets from various biomedical data repositories.</p>"},{"location":"api/data_sources/#base-classes","title":"Base Classes","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data sources.</p> <p>               Bases: <code>Exception</code></p> <p>Exception raised for data source related errors.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.base.DataSource.download","title":"download  <code>abstractmethod</code>","text":"<pre><code>download(source_config, data_dir)\n</code></pre> <p>Download data from the source.</p> <p>Args:     source_config: Configuration specific to this data source     data_dir: Directory to download data to</p> <p>Returns:     Path to downloaded data (or None if no single file)</p> <p>Raises:     DataSourceError: If download fails</p>"},{"location":"api/data_sources/#biomlbench.data_sources.base.DataSource.get_leaderboard","title":"get_leaderboard  <code>abstractmethod</code>","text":"<pre><code>get_leaderboard(source_config)\n</code></pre> <p>Get the public leaderboard for the benchmark.</p> <p>Args:     source_config: Configuration specific to this data source</p> <p>Returns:     DataFrame with columns: teamName, score, submissionDate</p> <p>Raises:     DataSourceError: If leaderboard cannot be retrieved</p>"},{"location":"api/data_sources/#biomlbench.data_sources.base.DataSource.get_human_baselines","title":"get_human_baselines","text":"<pre><code>get_human_baselines(source_config)\n</code></pre> <p>Extract human baseline performance data.</p> <p>Args:     source_config: Configuration specific to this data source</p> <p>Returns:     DataFrame with columns: team_name, score, human_type, source     Returns None if no human baselines available</p> <p>Raises:     DataSourceError: If human baseline extraction fails</p>"},{"location":"api/data_sources/#biomlbench.data_sources.base.DataSource.supports_human_baselines","title":"supports_human_baselines","text":"<pre><code>supports_human_baselines()\n</code></pre> <p>Check if this data source can provide human baseline data.</p>"},{"location":"api/data_sources/#available-data-sources","title":"Available Data Sources","text":""},{"location":"api/data_sources/#kaggle-data-source","title":"Kaggle Data Source","text":"<p>               Bases: <code>DataSource</code></p> <p>Data source for Kaggle competitions.</p> <p>Downloads competition data and leaderboards using the Kaggle API.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.kaggle.KaggleDataSource.validate_config","title":"validate_config","text":"<pre><code>validate_config(source_config)\n</code></pre> <p>Validate Kaggle source configuration.</p> <p>Args:     source_config: Should contain 'competition_id' key</p> <p>Returns:     True if valid</p> <p>Raises:     DataSourceConfigError: If configuration is invalid</p>"},{"location":"api/data_sources/#biomlbench.data_sources.kaggle.KaggleDataSource.download","title":"download","text":"<pre><code>download(source_config, data_dir)\n</code></pre> <p>Download competition data from Kaggle.</p> <p>Args:     source_config: Must contain 'competition_id'     data_dir: Directory to download data to</p> <p>Returns:     Path to downloaded zip file</p> <p>Raises:     DataSourceError: If download fails</p>"},{"location":"api/data_sources/#biomlbench.data_sources.kaggle.KaggleDataSource.get_leaderboard","title":"get_leaderboard","text":"<pre><code>get_leaderboard(source_config)\n</code></pre> <p>Get leaderboard from Kaggle competition.</p> <p>Args:     source_config: Must contain 'competition_id'</p> <p>Returns:     DataFrame with leaderboard data</p> <p>Raises:     DataSourceError: If leaderboard cannot be retrieved</p>"},{"location":"api/data_sources/#biomlbench.data_sources.kaggle.KaggleDataSource.get_human_baselines","title":"get_human_baselines","text":"<pre><code>get_human_baselines(source_config)\n</code></pre> <p>Extract human baselines from Kaggle public leaderboard.</p> <p>Filters the public leaderboard to identify likely human participants and categorizes them by performance level.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.kaggle.KaggleDataSource.supports_human_baselines","title":"supports_human_baselines","text":"<pre><code>supports_human_baselines()\n</code></pre> <p>Kaggle supports human baseline extraction from public leaderboards.</p>"},{"location":"api/data_sources/#polaris-data-source","title":"Polaris Data Source","text":"<p>               Bases: <code>DataSource</code></p> <p>Data source for Polaris Hub benchmarks.</p> <p>Downloads benchmark data and provides leaderboard information from the Polaris platform using the polarishub conda environment.</p>"},{"location":"api/data_sources/#biomlbench.data_sources.polaris.PolarisDataSource.validate_config","title":"validate_config","text":"<pre><code>validate_config(source_config)\n</code></pre> <p>Validate Polaris source configuration.</p> <p>Args:     source_config: Should contain 'benchmark_id' key</p> <p>Returns:     True if valid</p> <p>Raises:     DataSourceConfigError: If configuration is invalid</p>"},{"location":"api/data_sources/#biomlbench.data_sources.polaris.PolarisDataSource.download","title":"download","text":"<pre><code>download(source_config, data_dir)\n</code></pre> <p>Download benchmark data from Polaris Hub.</p> <p>Args:     source_config: Must contain 'benchmark_id'     data_dir: Directory to save data to</p> <p>Returns:     Path to the data directory (Polaris doesn't use zip files)</p> <p>Raises:     DataSourceError: If download fails</p>"},{"location":"api/data_sources/#biomlbench.data_sources.polaris.PolarisDataSource.get_leaderboard","title":"get_leaderboard","text":"<pre><code>get_leaderboard(source_config)\n</code></pre> <p>Get leaderboard from Polaris Hub.</p> <p>Note: Polaris doesn't provide direct leaderboard access via API. This returns a minimal leaderboard structure.</p> <p>Args:     source_config: Must contain 'benchmark_id'</p> <p>Returns:     DataFrame with minimal leaderboard data</p> <p>Raises:     DataSourceError: If leaderboard cannot be created</p>"},{"location":"api/data_sources/#factory-functions","title":"Factory Functions","text":"<p>Convenience function to create a data source instance.</p> <p>Args:     source_type: Type of data source to create</p> <p>Returns:     Configured data source instance</p> <p>Raises:     DataSourceNotFoundError: If source type is not registered</p> <p>Convenience function to list all available data source types.</p> <p>Returns:     List of registered data source type strings</p>"},{"location":"api/data_sources/#usage-examples","title":"Usage Examples","text":"<pre><code>from biomlbench.data_sources import create_data_source\n\n# Get Kaggle data source\nkaggle_source = create_data_source(\"kaggle\")\n\n# Download competition data\nkaggle_source.download(\n    source_config={\"competition_id\": \"histopathologic-cancer-detection\"},\n    data_dir=Path(\"/data\")\n)\n\n# Get leaderboard\nleaderboard = kaggle_source.get_leaderboard(\n    source_config={\"competition_id\": \"histopathologic-cancer-detection\"}\n)\n</code></pre>"},{"location":"api/grading/","title":"Grading API","text":"<p>The grading module provides evaluation and scoring functionality for agent submissions on biomedical tasks.</p>"},{"location":"api/grading/#core-functions","title":"Core Functions","text":"<p>Grades multiple submissions stored in a JSONL file. Saves the aggregated report as a JSON file.</p> <p>Grades a submission CSV for the given task.</p> <p>Validates a submission for the given task by actually running the task grader. This is designed for end users, not developers (we assume that the task grader is correctly implemented and use that for validating the submission, not the other way around).</p> <p>Builds the summary report from a list of task reports.</p>"},{"location":"api/grading/#grader-classes","title":"Grader Classes","text":"<p>Report for a single biomedical task evaluation.</p> <p>Extended from MLE-bench CompetitionReport with biomedical-specific fields.</p>"},{"location":"api/grading/#biomlbench.grade_helpers.Grader.is_lower_better","title":"is_lower_better","text":"<pre><code>is_lower_better(leaderboard)\n</code></pre> <p>Determines if a lower score is better based on the leaderboard. Returns True if lower scores are better, False otherwise.</p>"},{"location":"api/grading/#biomlbench.grade_helpers.Grader.rank_score","title":"rank_score","text":"<pre><code>rank_score(score, leaderboard)\n</code></pre> <p>Ranks a score based on the leaderboard. Returns a dictionary of bools with the following keys: - gold_medal: bool - silver_medal: bool - bronze_medal: bool - above_median: bool - gold_threshold: float - silver_threshold: float - bronze_threshold: float - median_threshold: float</p>"},{"location":"api/grading/#single-task-evaluation","title":"Single Task Evaluation","text":"<pre><code>from biomlbench.grade import grade_csv\nfrom biomlbench.registry import registry\n\n# Grade a single submission\ntask = registry.get_task(\"caco2-wang\")\nsubmission_path = Path(\"submission.csv\")\n\nreport = grade_csv(submission_path, task)\n\nprint(f\"Score: {report.score}\")\nprint(f\"Medal: {'\ud83e\udd47' if report.gold_medal else '\ud83e\udd48' if report.silver_medal else '\ud83e\udd49' if report.bronze_medal else '\u274c'}\")\nprint(f\"Beats human: {report.beats_human}\")\n</code></pre>"},{"location":"api/grading/#multi-task-evaluation","title":"Multi-Task Evaluation","text":"<pre><code>from biomlbench.grade import grade_jsonl\nfrom pathlib import Path\n\n# Grade multiple tasks from submission.jsonl\nsubmission_path = Path(\"runs/my-run-group/submission.jsonl\")\noutput_dir = Path(\"results/\")\n\ngrade_jsonl(submission_path, output_dir)\n# Creates timestamped grading report in output_dir\n</code></pre>"},{"location":"api/grading/#custom-graders","title":"Custom Graders","text":"<pre><code>from biomlbench.grade_helpers import Grader\nimport pandas as pd\n\n# Define custom metric\ndef rmse_metric(submission: pd.DataFrame, answers: pd.DataFrame) -&gt; float:\n    \"\"\"Root Mean Square Error for regression tasks.\"\"\"\n    y_true = answers['target'].values\n    y_pred = submission['prediction'].values\n    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n\n# Create grader\ngrader = Grader(name=\"rmse\", grade_fn=\"my_module.metrics:rmse_metric\")\n\n# Use grader\nscore = grader(submission_df, answers_df)\n</code></pre>"},{"location":"api/grading/#biomedical-metrics","title":"Biomedical Metrics","text":"<p>BioML-bench includes domain-specific evaluation metrics:</p>"},{"location":"api/grading/#medical-imaging","title":"Medical Imaging","text":"<ul> <li>AUC-ROC - Area under ROC curve</li> <li>Precision/Recall - Binary classification metrics</li> <li>Dice Coefficient - Segmentation overlap</li> </ul>"},{"location":"api/grading/#drug-discovery","title":"Drug Discovery","text":"<ul> <li>RMSE - Root mean square error</li> <li>R\u00b2 Score - Coefficient of determination  </li> <li>Spearman Correlation - Rank correlation</li> </ul>"},{"location":"api/grading/#protein-engineering","title":"Protein Engineering","text":"<ul> <li>RMSD - Root mean square deviation</li> <li>TM-score - Template modeling score</li> <li>GDT-TS - Global distance test</li> </ul>"},{"location":"api/grading/#human-performance-comparison","title":"Human Performance Comparison","text":"<pre><code>from biomlbench.grade import calculate_human_performance_metrics\n\n# Compare agent to human baselines\nagent_score = 0.89\nhuman_df = pd.read_csv(\"human_baselines.csv\")\nis_lower_better = False\n\nbeats_human, percentile = calculate_human_performance_metrics(\n    agent_score, human_df, is_lower_better\n)\n\nprint(f\"Agent beats human median: {beats_human}\")\nprint(f\"Agent percentile vs humans: {percentile}%\")\n</code></pre>"},{"location":"api/grading/#medal-system","title":"Medal System","text":"<p>BioML-bench uses a Kaggle-style medal system that varies based on leaderboard size:</p> <p>For small leaderboards (1-99 teams): - \ud83e\udd47 Gold: Top 10% of submissions - \ud83e\udd48 Silver: Top 20% (but not gold) - \ud83e\udd49 Bronze: Top 40% (but not silver/gold)</p> <p>For medium leaderboards (100-249 teams): - \ud83e\udd47 Gold: Top 10 positions (fixed) - \ud83e\udd48 Silver: Top 20% (but not gold) - \ud83e\udd49 Bronze: Top 40% (but not silver/gold)</p> <p>For large leaderboards (250-999 teams): - \ud83e\udd47 Gold: Top (10 + 0.2% of teams) positions - \ud83e\udd48 Silver: Top 50 positions (fixed) - \ud83e\udd49 Bronze: Top 100 positions (fixed)</p> <p>For very large leaderboards (1000+ teams): - \ud83e\udd47 Gold: Top (10 + 0.2% of teams) positions - \ud83e\udd48 Silver: Top 5% of submissions - \ud83e\udd49 Bronze: Top 10% of submissions</p> <p>Medal thresholds follow the official Kaggle competition progression system.</p>"},{"location":"api/grading/#submission-validation","title":"Submission Validation","text":"<pre><code>from biomlbench.grade import validate_submission\n\n# Check submission format\nis_valid, message = validate_submission(submission_path, task)\n\nif not is_valid:\n    print(f\"Submission error: {message}\")\nelse:\n    print(\"Submission is valid\")\n</code></pre>"},{"location":"api/grading/#report-generation","title":"Report Generation","text":"<p>TaskReport objects contain comprehensive evaluation results:</p> <pre><code>report = grade_csv(submission_path, task)\n\n# Access results\nprint(f\"Task: {report.task_id}\")\nprint(f\"Score: {report.score}\")\nprint(f\"Gold medal: {report.gold_medal}\")\nprint(f\"Above median: {report.above_median}\")\nprint(f\"Beats human: {report.beats_human}\")\nprint(f\"Human percentile: {report.human_percentile}\")\n\n# Convert to dictionary for JSON export\nreport_dict = report.to_dict()\n</code></pre>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>This section provides comprehensive API documentation for BioML-bench's Python modules and classes.</p>"},{"location":"api/overview/#core-modules","title":"Core Modules","text":""},{"location":"api/overview/#cli-biomlbenchcli","title":"CLI - <code>biomlbench.cli</code>","text":"<p>Command-line interface and argument parsing for all BioML-bench commands.</p>"},{"location":"api/overview/#registry-biomlbenchregistry","title":"Registry - <code>biomlbench.registry</code>","text":"<p>Task registry system for discovering, loading, and managing biomedical benchmark tasks.</p>"},{"location":"api/overview/#tasks-biomlbenchdata","title":"Tasks - <code>biomlbench.data</code>","text":"<p>Task preparation, data downloading, and dataset management functionality.</p>"},{"location":"api/overview/#agents-biomlbenchagents","title":"Agents - <code>biomlbench.agents</code>","text":"<p>Agent execution system for running AI agents in containerized environments.</p>"},{"location":"api/overview/#grading-biomlbenchgrade","title":"Grading - <code>biomlbench.grade</code>","text":"<p>Evaluation and scoring system for agent submissions with biomedical metrics.</p>"},{"location":"api/overview/#data-sources-biomlbenchdata_sources","title":"Data Sources - <code>biomlbench.data_sources</code>","text":"<p>Pluggable data source system supporting Kaggle, Polaris, and custom sources.</p>"},{"location":"api/overview/#utilities-biomlbenchutils","title":"Utilities - <code>biomlbench.utils</code>","text":"<p>Shared utility functions for file operations, logging, and data processing.</p>"},{"location":"api/overview/#key-classes","title":"Key Classes","text":""},{"location":"api/overview/#task","title":"Task","text":"<p>Represents a biomedical benchmark task with metadata, data paths, and evaluation configuration.</p> <pre><code>from biomlbench.registry import Task\n\ntask = registry.get_task(\"caco2-wang\")\nprint(f\"Task: {task.name}\")\nprint(f\"Domain: {task.domain}\")\nprint(f\"Type: {task.task_type}\")\n</code></pre>"},{"location":"api/overview/#registry","title":"Registry","text":"<p>Central registry for discovering and managing tasks across different biomedical domains.</p> <pre><code>from biomlbench.registry import registry\n\n# List all available tasks\ntasks = registry.list_task_ids()\n\n# Get tasks by domain\noncology_tasks = registry.get_tasks_by_domain(\"oncology\")\n\n# Get tasks by type\nimaging_tasks = registry.get_tasks_by_type(\"medical_imaging\")\n</code></pre>"},{"location":"api/overview/#agent","title":"Agent","text":"<p>Represents an AI agent with execution configuration and container settings.</p> <pre><code>from agents.registry import registry as agent_registry\n\nagent = agent_registry.get_agent(\"dummy\")\nprint(f\"Agent: {agent.name}\")\nprint(f\"Privileged: {agent.privileged}\")\n</code></pre>"},{"location":"api/overview/#grader","title":"Grader","text":"<p>Handles task-specific evaluation metrics and scoring logic.</p> <pre><code>from biomlbench.grade_helpers import Grader\n\ngrader = Grader(name=\"auc-roc\", grade_fn=\"biomlbench.metrics.auc_roc\")\nscore = grader(submission_df, answers_df)\n</code></pre>"},{"location":"api/overview/#data-source-architecture","title":"Data Source Architecture","text":"<p>BioML-bench uses a pluggable data source system:</p> <pre><code>from biomlbench.data_sources import get_data_source\n\n# Get Kaggle data source\nkaggle_source = get_data_source(\"kaggle\")\n\n# Download task data\nkaggle_source.download(\n    source_config={\"competition_id\": \"histopathologic-cancer-detection\"},\n    data_dir=Path(\"/data\")\n)\n\n# Get leaderboard\nleaderboard = kaggle_source.get_leaderboard(\n    source_config={\"competition_id\": \"histopathologic-cancer-detection\"}\n)\n</code></pre>"},{"location":"api/overview/#task-configuration","title":"Task Configuration","text":"<p>Tasks are configured via YAML files with biomedical-specific metadata:</p> <pre><code>id: caco2-wang\nname: \"Caco-2 Permeability Prediction\"\ntask_type: drug_discovery\ndomain: pharmacokinetics\ndifficulty: medium\n\ndataset:\n  answers: caco2-wang/prepared/private/answers.csv\n  sample_submission: caco2-wang/prepared/public/sample_submission.csv\n\ngrader:\n  name: rmse\n  grade_fn: biomlbench.tasks.caco2-wang.grade:grade\n\nbiomedical_metadata:\n  modality: \"molecular_properties\"\n  data_type: \"regression\"\n  clinical_relevance: \"drug_absorption\"\n</code></pre>"},{"location":"api/overview/#error-handling","title":"Error Handling","text":"<p>BioML-bench defines custom exceptions for different error conditions:</p> <pre><code>from biomlbench.data_sources.base import DataSourceError\nfrom biomlbench.grade_helpers import InvalidSubmissionError\n\ntry:\n    score = grader(submission, answers)\nexcept InvalidSubmissionError as e:\n    logger.warning(f\"Invalid submission: {e}\")\nexcept DataSourceError as e:\n    logger.error(f\"Data source error: {e}\")\n</code></pre>"},{"location":"api/overview/#logging","title":"Logging","text":"<p>BioML-bench uses structured logging throughout:</p> <pre><code>from biomlbench.utils import get_logger\n\nlogger = get_logger(__name__)\nlogger.info(\"Task preparation starting\")\nlogger.warning(\"Missing optional dependency\")\nlogger.error(\"Task preparation failed\")\n</code></pre>"},{"location":"api/overview/#extension-points","title":"Extension Points","text":""},{"location":"api/overview/#adding-data-sources","title":"Adding Data Sources","text":"<p>Implement the <code>DataSource</code> interface:</p> <pre><code>from biomlbench.data_sources.base import DataSource\n\nclass MyDataSource(DataSource):\n    def download(self, source_config, data_dir):\n        # Implementation\n        pass\n\n    def get_leaderboard(self, source_config):\n        # Implementation  \n        pass\n</code></pre>"},{"location":"api/overview/#adding-metrics","title":"Adding Metrics","text":"<p>Implement grading functions:</p> <pre><code>def my_metric(submission: pd.DataFrame, answers: pd.DataFrame) -&gt; float:\n    \"\"\"Custom biomedical metric.\"\"\"\n    # Implementation\n    return score\n</code></pre>"},{"location":"api/overview/#adding-tasks","title":"Adding Tasks","text":"<p>Create task directory with required files:</p> <pre><code>biomlbench/tasks/my-task/\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 prepare.py  \n\u251c\u2500\u2500 grade.py\n\u2514\u2500\u2500 description.md\n</code></pre>"},{"location":"api/overview/#type-hints","title":"Type Hints","text":"<p>BioML-bench uses type hints throughout for better IDE support:</p> <pre><code>from typing import List, Optional, Dict, Any\nfrom pathlib import Path\nimport pandas as pd\n\ndef process_task(\n    task_id: str,\n    data_dir: Optional[Path] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Process a biomedical task with type safety.\"\"\"\n    pass\n</code></pre>"},{"location":"api/registry/","title":"Registry API","text":"<p>The registry system manages discovery, loading, and organization of biomedical benchmark tasks.</p>"},{"location":"api/registry/#registry-class","title":"Registry Class","text":""},{"location":"api/registry/#biomlbench.registry.Registry.get_task","title":"get_task","text":"<pre><code>get_task(task_id)\n</code></pre> <p>Fetch the task from the registry.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_tasks_dir","title":"get_tasks_dir","text":"<pre><code>get_tasks_dir()\n</code></pre> <p>Retrieves the tasks directory within the registry.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_splits_dir","title":"get_splits_dir","text":"<pre><code>get_splits_dir()\n</code></pre> <p>Retrieves the splits directory within the repository.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_tasks_by_domain","title":"get_tasks_by_domain","text":"<pre><code>get_tasks_by_domain(domain)\n</code></pre> <p>List all task IDs for a specific biomedical domain.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_tasks_by_type","title":"get_tasks_by_type","text":"<pre><code>get_tasks_by_type(task_type)\n</code></pre> <p>List all task IDs for a specific task type.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_lite_task_ids","title":"get_lite_task_ids","text":"<pre><code>get_lite_task_ids()\n</code></pre> <p>List all task IDs for the lite version (low difficulty tasks).</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_tasks_by_difficulty","title":"get_tasks_by_difficulty","text":"<pre><code>get_tasks_by_difficulty(difficulty)\n</code></pre> <p>List all task IDs for a specific difficulty level.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.get_data_dir","title":"get_data_dir","text":"<pre><code>get_data_dir()\n</code></pre> <p>Retrieves the data directory within the registry.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.set_data_dir","title":"set_data_dir","text":"<pre><code>set_data_dir(new_data_dir)\n</code></pre> <p>Sets the data directory within the registry.</p>"},{"location":"api/registry/#biomlbench.registry.Registry.list_task_ids","title":"list_task_ids","text":"<pre><code>list_task_ids()\n</code></pre> <p>List all task IDs available in the registry, sorted alphabetically.</p>"},{"location":"api/registry/#task-class","title":"Task Class","text":"<p>Represents a biomedical ML task in the BioML-bench framework.</p> <p>Extended from MLE-bench Competition class with biomedical-specific metadata.</p>"},{"location":"api/registry/#global-registry-instance","title":"Global Registry Instance","text":"<p>The default registry instance provides immediate access to tasks:</p> <pre><code>from biomlbench.registry import registry\n\n# Get task information\ntask = registry.get_task(\"caco2-wang\")\nprint(f\"Task: {task.name} ({task.domain})\")\n\n# List all tasks\nall_tasks = registry.list_task_ids()\nprint(f\"Available tasks: {len(all_tasks)}\")\n\n# Filter by domain\noncology_tasks = registry.get_tasks_by_domain(\"oncology\")\ndrug_discovery_tasks = registry.get_tasks_by_domain(\"drug_discovery\")\n\n# Filter by task type\nimaging_tasks = registry.get_tasks_by_type(\"medical_imaging\")\nprotein_tasks = registry.get_tasks_by_type(\"protein_engineering\")\n</code></pre>"},{"location":"api/registry/#task-discovery","title":"Task Discovery","text":"<p>Tasks are automatically discovered from the <code>biomlbench/tasks/</code> directory:</p> <pre><code>biomlbench/tasks/\n\u251c\u2500\u2500 caco2-wang/\n\u2502   \u251c\u2500\u2500 config.yaml          # Task configuration\n\u2502   \u251c\u2500\u2500 prepare.py           # Data preparation\n\u2502   \u251c\u2500\u2500 grade.py            # Evaluation logic\n\u2502   \u2514\u2500\u2500 description.md      # Task description\n\u2514\u2500\u2500 histopathologic-cancer-detection/\n    \u251c\u2500\u2500 config.yaml\n    \u251c\u2500\u2500 prepare.py\n    \u251c\u2500\u2500 grade.py\n    \u2514\u2500\u2500 description.md\n</code></pre>"},{"location":"api/registry/#task-configuration-format","title":"Task Configuration Format","text":"<p>Each task requires a <code>config.yaml</code> file:</p> <pre><code>id: caco2-wang\nname: \"Caco-2 Cell Permeability Prediction\"\ntask_type: drug_discovery\ndomain: admet\ndifficulty: medium\nawards_medals: true\nprizes: null\ndescription: biomlbench/tasks/caco2-wang/description.md\n\n# Data source configuration\ndata_source:\n  type: polaris\n  benchmark_id: tdcommons/caco2-wang\n\ndataset:\n  answers: caco2-wang/prepared/private/answers.csv\n  sample_submission: caco2-wang/prepared/public/sample_submission.csv\n\ngrader:\n  name: mean-absolute-error\n  grade_fn: biomlbench.tasks.caco2-wang.grade:grade\n\npreparer: biomlbench.tasks.caco2-wang.prepare:prepare\n\n# Biomedical-specific metadata\nbiomedical_metadata:\n  modality: \"molecular\"\n  data_type: \"smiles_regression\"\n  clinical_relevance: \"drug_absorption\"\n  target_property: \"caco2_permeability\"\n  original_source: \"polaris_hub\"\n  difficulty_factors:\n    - \"molecular_complexity\"\n    - \"limited_training_data\"\n    - \"admet_prediction_uncertainty\"\n\n# Human baselines are extracted dynamically from leaderboard data\n# via the data source (Polaris Hub publications, Kaggle leaderboards, etc.)\n\n# Computational requirements  \ncompute_requirements:\n  recommended_gpu_memory_gb: 4\n  estimated_runtime_minutes: 15\n  max_dataset_size_gb: 1\n</code></pre>"},{"location":"api/registry/#data-directory-management","title":"Data Directory Management","text":"<p>The registry supports configurable data directories:</p> <pre><code>from pathlib import Path\nfrom biomlbench.registry import Registry\n\n# Use custom data directory\ncustom_registry = Registry(data_dir=Path(\"/custom/data\"))\n\n# Or set on existing registry\nnew_registry = registry.set_data_dir(Path(\"/new/data\"))\n\n# Get current data directory\ndata_dir = registry.get_data_dir()\nprint(f\"Data stored in: {data_dir}\")\n</code></pre>"},{"location":"api/registry/#task-filtering-and-organization","title":"Task Filtering and Organization","text":""},{"location":"api/registry/#by-biomedical-domain","title":"By Biomedical Domain","text":"<pre><code># Get tasks by specific domains\noncology_tasks = registry.get_tasks_by_domain(\"oncology\")\ncardiology_tasks = registry.get_tasks_by_domain(\"cardiology\")\nneurology_tasks = registry.get_tasks_by_domain(\"neurology\")\n\n# Available domains\ndomains = set()\nfor task_id in registry.list_task_ids():\n    task = registry.get_task(task_id)\n    domains.add(task.domain)\nprint(f\"Domains: {sorted(domains)}\")\n</code></pre>"},{"location":"api/registry/#by-task-type","title":"By Task Type","text":"<pre><code># Get tasks by type\nimaging_tasks = registry.get_tasks_by_type(\"medical_imaging\")\ndrug_tasks = registry.get_tasks_by_type(\"drug_discovery\")\nprotein_tasks = registry.get_tasks_by_type(\"protein_engineering\")\n\n# Available task types\ntask_types = set()\nfor task_id in registry.list_task_ids():\n    task = registry.get_task(task_id)\n    task_types.add(task.task_type)\nprint(f\"Task types: {sorted(task_types)}\")\n</code></pre>"},{"location":"api/registry/#by-difficulty","title":"By Difficulty","text":"<pre><code># Get tasks by difficulty level\neasy_tasks = registry.get_tasks_by_difficulty(\"easy\")\nmedium_tasks = registry.get_tasks_by_difficulty(\"medium\")\nhard_tasks = registry.get_tasks_by_difficulty(\"hard\")\n\n# Get lite task set (easy + medium)\nlite_tasks = registry.get_lite_task_ids()\n</code></pre>"},{"location":"api/registry/#task-metadata-access","title":"Task Metadata Access","text":""},{"location":"api/registry/#biomedical-metadata","title":"Biomedical Metadata","text":"<pre><code>task = registry.get_task(\"histopathologic-cancer-detection\")\n\n# Access biomedical-specific metadata\nmetadata = task.biomedical_metadata\nprint(f\"Modality: {metadata['modality']}\")\nprint(f\"Organ system: {metadata['organ_system']}\")\nprint(f\"Clinical relevance: {metadata['clinical_relevance']}\")\n\n# Human baseline performance\nif task.human_baselines:\n    for role, score in task.human_baselines.items():\n        print(f\"{role}: {score}\")\n\n# Computational requirements\nif task.compute_requirements:\n    gpu_mem = task.compute_requirements.get('recommended_gpu_memory_gb', 0)\n    runtime = task.compute_requirements.get('estimated_runtime_minutes', 0)\n    print(f\"GPU: {gpu_mem}GB, Runtime: {runtime}min\")\n</code></pre>"},{"location":"api/registry/#data-source-information","title":"Data Source Information","text":"<pre><code>task = registry.get_task(\"caco2-wang\")\n\n# Data source configuration\nif task.data_source:\n    source_type = task.data_source['type']\n    print(f\"Data source: {source_type}\")\n\n    if source_type == \"kaggle\":\n        comp_id = task.data_source['competition_id']\n        print(f\"Kaggle competition: {comp_id}\")\n    elif source_type == \"polaris\":\n        benchmark_id = task.data_source['benchmark_id']\n        print(f\"Polaris benchmark: {benchmark_id}\")\n</code></pre>"},{"location":"api/registry/#file-path-resolution","title":"File Path Resolution","text":"<p>The registry automatically resolves file paths for task components:</p> <pre><code>task = registry.get_task(\"caco2-wang\")\n\n# Data file paths\nprint(f\"Answers: {task.answers}\")\nprint(f\"Sample submission: {task.sample_submission}\")\n\n# Directory paths\nprint(f\"Raw data: {task.raw_dir}\")\nprint(f\"Public data: {task.public_dir}\")\nprint(f\"Private data: {task.private_dir}\")\n\n# Verification files\nprint(f\"Checksums: {task.checksums}\")\nprint(f\"Leaderboard: {task.leaderboard}\")\n</code></pre>"},{"location":"api/registry/#task-splits-and-collections","title":"Task Splits and Collections","text":""},{"location":"api/registry/#predefined-splits","title":"Predefined Splits","text":"<pre><code># Get tasks from predefined splits\nlite_tasks = registry.get_lite_task_ids()\n\n# Check if splits directory exists\nsplits_dir = registry.get_splits_dir()\nif (splits_dir / \"medical-imaging.txt\").exists():\n    with open(splits_dir / \"medical-imaging.txt\") as f:\n        imaging_split = [line.strip() for line in f]\n</code></pre>"},{"location":"api/registry/#custom-task-collections","title":"Custom Task Collections","text":"<pre><code># Create custom task collections\nmolecular_tasks = []\nfor task_id in registry.list_task_ids():\n    task = registry.get_task(task_id)\n    if \"molecular\" in task.biomedical_metadata.get(\"modality\", \"\"):\n        molecular_tasks.append(task_id)\n\n# Filter by clinical relevance\ndrug_absorption_tasks = []\nfor task_id in registry.list_task_ids():\n    task = registry.get_task(task_id)\n    if task.biomedical_metadata.get(\"clinical_relevance\") == \"drug_absorption\":\n        drug_absorption_tasks.append(task_id)\n</code></pre>"},{"location":"api/registry/#error-handling","title":"Error Handling","text":"<pre><code>from biomlbench.registry import registry\n\ntry:\n    task = registry.get_task(\"nonexistent-task\")\nexcept ValueError as e:\n    print(f\"Task not found: {e}\")\n\ntry:\n    # Invalid task configuration\n    task = registry.get_task(\"invalid-task\")\nexcept ValueError as e:\n    print(f\"Invalid task config: {e}\")\n</code></pre>"},{"location":"api/registry/#extending-the-registry","title":"Extending the Registry","text":""},{"location":"api/registry/#adding-new-tasks","title":"Adding New Tasks","text":"<ol> <li> <p>Create task directory structure:    <pre><code>mkdir -p biomlbench/tasks/my-new-task\n</code></pre></p> </li> <li> <p>Add required files:</p> </li> <li><code>config.yaml</code> - Task configuration</li> <li><code>prepare.py</code> - Data preparation logic</li> <li><code>grade.py</code> - Evaluation function</li> <li> <p><code>description.md</code> - Task description</p> </li> <li> <p>Task is automatically discovered on next registry access</p> </li> </ol>"},{"location":"api/registry/#custom-registry-implementations","title":"Custom Registry Implementations","text":"<pre><code>from biomlbench.registry import Registry\nfrom pathlib import Path\n\nclass CustomRegistry(Registry):\n    def get_custom_tasks(self) -&gt; list[str]:\n        \"\"\"Get tasks with custom criteria.\"\"\"\n        return [\n            task_id for task_id in self.list_task_ids()\n            if self.get_task(task_id).difficulty == \"easy\"\n        ]\n\n# Use custom registry\ncustom_registry = CustomRegistry(data_dir=Path(\"/data\"))\neasy_tasks = custom_registry.get_custom_tasks()\n</code></pre>"},{"location":"api/tasks/","title":"Tasks and Data API","text":"<p>The data module handles task preparation, dataset downloading, and data management for biomedical benchmark tasks.</p>"},{"location":"api/tasks/#core-functions","title":"Core Functions","text":"<p>Download and prepare a dataset using the appropriate data source.</p> <p>Args:     task: Task to prepare     keep_raw: Whether to keep raw downloaded data     overwrite_checksums: Whether to overwrite existing checksums     overwrite_leaderboard: Whether to overwrite existing leaderboard     skip_verification: Whether to skip checksum verification</p> <p>Checks if the task has non-empty <code>public</code> and <code>private</code> directories with the expected files.</p> <p>Ensures the leaderboard for a given task exists.</p> <p>Args:     task: Task to ensure leaderboard for     force: Whether to force download/update of leaderboard</p> <p>Returns:     Path to the leaderboard file</p> <p>Raises:     FileNotFoundError: If leaderboard cannot be found or created</p> <p>Load leaderboard data for a task.</p> <p>Prepare human baseline data for a task.</p> <p>Args:     task: Task to prepare human baselines for     force: Whether to force re-download of human baselines</p> <p>Returns:     Path to human baselines CSV file, or None if not available</p>"},{"location":"api/tasks/#task-preparation-workflow","title":"Task Preparation Workflow","text":"<pre><code>from biomlbench.registry import registry\nfrom biomlbench.data import download_and_prepare_dataset, is_dataset_prepared\n\n# Get task from registry\ntask = registry.get_task(\"caco2-wang\")\n\n# Check if already prepared\nif not is_dataset_prepared(task):\n    # Download and prepare dataset\n    download_and_prepare_dataset(\n        task=task,\n        keep_raw=False,\n        skip_verification=False\n    )\n\nprint(f\"Task prepared: {task.name}\")\n</code></pre>"},{"location":"api/tasks/#data-directory-structure","title":"Data Directory Structure","text":"<p>After preparation, tasks have the following structure:</p> <pre><code>data/\n\u2514\u2500\u2500 task-id/\n    \u251c\u2500\u2500 raw/                    # Raw downloaded data\n    \u251c\u2500\u2500 prepared/\n    \u2502   \u251c\u2500\u2500 public/            # Data accessible to agents\n    \u2502   \u2502   \u251c\u2500\u2500 train.csv\n    \u2502   \u2502   \u251c\u2500\u2500 test_features.csv\n    \u2502   \u2502   \u251c\u2500\u2500 sample_submission.csv\n    \u2502   \u2502   \u2514\u2500\u2500 description.md\n    \u2502   \u2514\u2500\u2500 private/           # Private evaluation data\n    \u2502       \u2514\u2500\u2500 answers.csv\n    \u2514\u2500\u2500 checksums.yaml         # Data integrity verification\n</code></pre>"},{"location":"api/tasks/#data-source-integration","title":"Data Source Integration","text":"<p>Tasks can use different data sources:</p> <pre><code># Kaggle competition data\ntask_config = {\n    \"data_source\": {\n        \"type\": \"kaggle\",\n        \"competition_id\": \"histopathologic-cancer-detection\"\n    }\n}\n\n# Polaris molecular datasets\ntask_config = {\n    \"data_source\": {\n        \"type\": \"polaris\",\n        \"dataset_id\": \"Caco2_Wang\"\n    }\n}\n</code></pre>"},{"location":"api/tasks/#human-baseline-management","title":"Human Baseline Management","text":"<pre><code>from biomlbench.data import prepare_human_baselines\n\n# Extract human performance data\ntask = registry.get_task(\"histopathologic-cancer-detection\")\nprepare_human_baselines(task, force=True)\n\n# Human baselines are saved as CSV files\nhuman_baselines_path = task.public_dir / \"human_baselines.csv\"\n</code></pre>"},{"location":"api/tasks/#verification-and-validation","title":"Verification and Validation","text":"<p>The module provides data integrity checking:</p> <ul> <li>Checksums - Verify downloaded data integrity</li> <li>Format validation - Ensure CSV files have correct structure</li> <li>Completeness checks - Verify all required files are present </li> </ul>"},{"location":"api/utils/","title":"Utilities API","text":"<p>Shared utility functions for file operations, logging, data processing, and common operations.</p>"},{"location":"api/utils/#core-functions","title":"Core Functions","text":"<p>Returns an absolute path to the directory storing runs.</p> <p>Creates a directory for the run.</p> <p>Generate a submission.jsonl file from agent run metadata.</p> <p>This function reads the metadata.json file created by run_agent_async() and creates a JSONL file mapping task IDs to their submission file paths, which can be used directly with the <code>biomlbench grade</code> command.</p> <p>Args:     metadata_path: Path to the metadata.json file     output_path: Path for the output submission.jsonl file (defaults to same directory as metadata)     rel_log_path: Path to logfile relative to run directory     rel_code_path: Path to code file relative to run directory</p> <p>Returns:     Path to the generated submission.jsonl file</p>"},{"location":"api/utils/#file-operations","title":"File Operations","text":"<p>Reads a CSV file and returns a DataFrame with custom default kwargs.</p> <p>Read a JSONL file and return a list of dictionaries of its content.</p> <p>Args:     file_path (str): Path to the JSONL file.     skip_commented_out_lines (bool): If True, skip commented out lines.</p> <p>Returns:     list[dict]: List of dictionaries parsed from the JSONL file.</p> <p>Loads a YAML file and returns its contents as a dictionary.</p> <p>Compresses the contents of a source directory to a compressed file.</p> <p>Extracts the contents of a compressed file to a destination directory.</p>"},{"location":"api/utils/#path-utilities","title":"Path Utilities","text":"<p>Returns an absolute path to the BioML-bench module.</p> <p>Returns an absolute path to the repository directory.</p>"},{"location":"api/utils/#import-and-module-functions","title":"Import and Module Functions","text":"<p>Imports a function from a module given a string in the format <code>potentially.nested.module_name:fn_name</code>.</p> <p>Basically equivalent to <code>from potentially.nested.module_name import fn_name</code>.</p> <p>Retrieves the file path of the module where the given callable is defined.</p> <p>Args:     callable (Callable): The callable for which the module path is required.</p> <p>Returns:     Path: The relative path to the module file from the current working directory.</p> <p>Raises:     AssertionError: If the module does not have a file path.</p>"},{"location":"api/utils/#kaggle-integration","title":"Kaggle Integration","text":"<p>Authenticates the Kaggle API and returns an authenticated API object, or raises an error if authentication fails.</p>"},{"location":"components/agents/","title":"Agents Component","text":"<p>The agents system provides a framework for registering, building, and executing AI agents on biomedical tasks.</p>"},{"location":"components/agents/#available-agents","title":"Available Agents","text":"<p>BioML-bench includes several reference agents:</p>"},{"location":"components/agents/#dummy-agent","title":"Dummy Agent","text":"<p>A simple test agent for environment validation.</p>"},{"location":"components/agents/#aide-agent","title":"AIDE Agent","text":"<p>Advanced AI agent for automated biomedical research.</p>"},{"location":"components/agents/#mlagentbench","title":"MLAgentBench","text":"<p>Research agent for machine learning tasks.</p>"},{"location":"components/agents/#openhands-opendevin","title":"OpenHands (OpenDevin)","text":"<p>Code-writing agent with container management.</p>"},{"location":"components/agents/#agent-development","title":"Agent Development","text":"<p>See the updated agents README for:</p> <ul> <li>Agent architecture</li> <li>Building custom agents</li> <li>Container requirements</li> <li>Security considerations </li> </ul>"},{"location":"components/environment/","title":"Environment Reference","text":"<p>The <code>environment/</code> directory contains the containerized execution environment for BioML-bench agents, including the base Docker image, grading server, and configuration files.</p>"},{"location":"components/environment/#overview","title":"Overview","text":"<p>BioML-bench uses Docker containers to provide secure, isolated execution environments for AI agents. The environment includes:</p> <ul> <li>Base Docker image with biomedical libraries</li> <li>Grading server for submission validation</li> <li>Container configuration system</li> <li>Security isolation mechanisms</li> </ul>"},{"location":"components/environment/#core-components","title":"Core Components","text":""},{"location":"components/environment/#base-docker-image-dockerfile","title":"Base Docker Image (<code>Dockerfile</code>)","text":"<p>The foundational Docker image provides a complete biomedical ML environment:</p> <p>Base System: - Ubuntu 22.04 LTS - Python 3.11 via Miniconda - Essential system packages and development tools</p> <p>Biomedical Libraries: - RDKit - Molecular informatics toolkit - BioPython - Biological computation library - scikit-learn - Machine learning framework - pandas, numpy - Data processing</p> <p>ML Frameworks: - TensorFlow 2.17 with CUDA support - PyTorch 2.2.0 with GPU acceleration - Transformers for NLP models - OpenCV for computer vision</p> <p>Environment Structure: <pre><code>FROM ubuntu:22.04\n\n# System packages and Python\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3 python3-pip python3-venv \\\n    build-essential git curl wget \\\n    libsm6 libxext6 ffmpeg\n\n# Miniconda installation\nRUN wget miniconda.sh &amp;&amp; bash miniconda.sh -b -p /opt/conda\n\n# Create isolated environments\nRUN conda create -n agent python=3.11     # Agent execution\nRUN conda create -n biomlb python=3.11    # Grading server\n\n# Install biomedical dependencies\nRUN conda install -n agent -c conda-forge \\\n    rdkit biopython scikit-learn pandas numpy\n</code></pre></p>"},{"location":"components/environment/#grading-server-grading_serverpy","title":"Grading Server (<code>grading_server.py</code>)","text":"<p>A Flask-based validation server that runs inside containers to evaluate agent submissions:</p> <p>Key Features: - HTTP health check endpoint (<code>/health</code>) - Submission validation against private answers - Real-time evaluation feedback - Resource monitoring and limits</p> <p>Usage in Containers: <pre><code># Server starts automatically via entrypoint.sh\n# Agents can check health status\ncurl http://localhost:5000/health\n\n# Server validates submissions internally\n# No direct agent interaction required\n</code></pre></p>"},{"location":"components/environment/#container-entrypoint-entrypointsh","title":"Container Entrypoint (<code>entrypoint.sh</code>)","text":"<p>The entry script that configures the container environment and starts services:</p> <p>Responsibilities: - User environment setup (non-root execution) - Directory permissions configuration - Grading server initialization - Agent execution orchestration</p> <p>Execution Flow: 1. Validate container environment 2. Set up user permissions and directories 3. Start grading server in background 4. Wait for server health check 5. Execute agent start script 6. Clean up resources on exit</p> <p>Security Features: - Non-root user execution (<code>nonroot</code>) - Private directory isolation (<code>/private/</code>) - Read-only data mounts - Resource limit enforcement</p>"},{"location":"components/environment/#task-instructions","title":"Task Instructions","text":"<p>Standardized instructions provided to agents:</p>"},{"location":"components/environment/#instructionstxt","title":"<code>instructions.txt</code>","text":"<p>Complete task instructions with full context and examples.</p>"},{"location":"components/environment/#instructions_obfuscatedtxt","title":"<code>instructions_obfuscated.txt</code>","text":"<p>Minimal instructions to prevent overfitting and data leakage.</p> <p>Instruction Format: <pre><code># Task: [Task Name]\n\n## Objective\n[Clear description of the biomedical problem]\n\n## Data Description\n- train.csv: Training data with features and targets\n- test_features.csv: Test data (features only)\n- sample_submission.csv: Expected submission format\n\n## Evaluation Metric\n[Domain-specific metric: AUC-ROC, RMSE, etc.]\n\n## Submission Format\nCSV file with columns: [id, prediction]\n\n## Important Notes\n- No external data sources allowed\n- Use only provided training data\n- Submit predictions for all test samples\n</code></pre></p>"},{"location":"components/environment/#validation-script-validate_submissionsh","title":"Validation Script (<code>validate_submission.sh</code>)","text":"<p>Shell script for basic submission format validation:</p> <pre><code>#!/bin/bash\n# Validate submission file exists and has correct format\n\nSUBMISSION_FILE=\"/home/submission/submission.csv\"\n\nif [[ ! -f \"$SUBMISSION_FILE\" ]]; then\n    echo \"ERROR: submission.csv not found\"\n    exit 1\nfi\n\n# Check CSV format\nif ! head -1 \"$SUBMISSION_FILE\" | grep -q \"id.*prediction\"; then\n    echo \"ERROR: Invalid CSV headers\"\n    exit 1\nfi\n\necho \"Submission validation passed\"\n</code></pre>"},{"location":"components/environment/#container-configuration","title":"Container Configuration","text":""},{"location":"components/environment/#default-configuration-configcontainer_configsdefaultjson","title":"Default Configuration (<code>config/container_configs/default.json</code>)","text":"<pre><code>{\n    \"mem_limit\": null,        # No memory limit (use system default)\n    \"shm_size\": \"4G\",        # Shared memory for large datasets\n    \"nano_cpus\": 4000000000  # 4 CPU cores\n}\n</code></pre>"},{"location":"components/environment/#custom-configuration-options","title":"Custom Configuration Options","text":"<p>Resource Limits: <pre><code>{\n    \"mem_limit\": \"8g\",           # 8GB memory limit\n    \"shm_size\": \"4g\",           # 4GB shared memory\n    \"nano_cpus\": 8000000000,    # 8 CPU cores\n    \"gpus\": -1,                 # All available GPUs\n    \"runtime\": \"sysbox-runc\"    # Enhanced security runtime\n}\n</code></pre></p> <p>Security Settings: <pre><code>{\n    \"privileged\": false,         # Disable privileged mode\n    \"user\": \"nonroot\",          # Non-root user execution\n    \"read_only\": false,         # Allow container writes\n    \"security_opt\": [           # Security options\n        \"no-new-privileges:true\"\n    ]\n}\n</code></pre></p> <p>Volume Mounting: <pre><code>{\n    \"volumes\": {\n        \"/host/data\": {\n            \"bind\": \"/home/data\",\n            \"mode\": \"ro\"            # Read-only data access\n        },\n        \"/host/output\": {\n            \"bind\": \"/home/submission\",\n            \"mode\": \"rw\"            # Write access for submissions\n        }\n    }\n}\n</code></pre></p>"},{"location":"components/environment/#environment-variables","title":"Environment Variables","text":""},{"location":"components/environment/#system-environment","title":"System Environment","text":"<p>Set automatically by the container:</p> <pre><code># Directory paths\nDATA_DIR=\"/home/data\"\nSUBMISSION_DIR=\"/home/submission\"\nLOGS_DIR=\"/home/logs\"\nCODE_DIR=\"/home/code\"\nAGENT_DIR=\"/home/agent\"\n\n# Python environment\nPYTHONPATH=\"/opt/conda/envs/agent/lib/python3.11/site-packages\"\nCONDA_DEFAULT_ENV=\"agent\"\n\n# Task information\nTASK_ID=\"caco2-wang\"            # Set by agent runner\nTASK_TYPE=\"drug_discovery\"      # Biomedical task type\n</code></pre>"},{"location":"components/environment/#agent-specific-environment","title":"Agent-Specific Environment","text":"<p>Configurable via agent configuration:</p> <pre><code># API keys (for agents that need them)\nOPENAI_API_KEY=\"sk-...\"\nANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Agent configuration\nAGENT_TIMEOUT=\"3600\"            # 1 hour timeout\nMAX_ITERATIONS=\"10\"             # Iteration limit\nVERBOSE=\"true\"                  # Enable verbose logging\n</code></pre>"},{"location":"components/environment/#security-model","title":"Security Model","text":""},{"location":"components/environment/#isolation-mechanisms","title":"Isolation Mechanisms","text":"<p>Container Isolation: - Separate network namespace - Isolated filesystem - Resource limits and quotas - User namespace isolation</p> <p>Data Protection: - Private answers in protected directory (<code>/private/</code>) - Read-only access to task data - No network access during execution - Temporary filesystem for scratch space</p> <p>Process Security: - Non-root user execution - Limited capabilities - No privileged operations - Process monitoring and limits</p>"},{"location":"components/environment/#sysbox-runtime-recommended","title":"Sysbox Runtime (Recommended)","text":"<p>Enhanced container security using Sysbox:</p> <p>Benefits: - VM-like isolation - Secure nested containers - Better resource management - Reduced attack surface</p> <p>Configuration: <pre><code>{\n    \"runtime\": \"sysbox-runc\",\n    \"security_opt\": [\n        \"systempaths=unconfined\"\n    ]\n}\n</code></pre></p>"},{"location":"components/environment/#privileged-containers-special-cases","title":"Privileged Containers (Special Cases)","text":"<p>Some agents (e.g., OpenHands) require privileged access:</p> <p>Requirements: - Set <code>I_ACCEPT_RUNNING_PRIVILEGED_CONTAINERS=True</code> - Understand security implications - Use only in trusted environments</p> <p>Configuration: <pre><code># agents/opendevin/config.yaml\nopendevin:\n  privileged: true\n  start: opendevin/start.sh\n  dockerfile: opendevin/Dockerfile\n</code></pre></p>"},{"location":"components/environment/#volume-mounting-strategy","title":"Volume Mounting Strategy","text":""},{"location":"components/environment/#data-volume-structure","title":"Data Volume Structure","text":"<pre><code>/home/data/                     # Task data (read-only)\n\u251c\u2500\u2500 description.md              # Task description\n\u251c\u2500\u2500 train.csv                  # Training data\n\u251c\u2500\u2500 test_features.csv          # Test features\n\u251c\u2500\u2500 sample_submission.csv      # Expected format\n\u2514\u2500\u2500 human_baselines.csv        # Human performance (if available)\n\n/home/submission/              # Agent output (read-write)\n\u2514\u2500\u2500 submission.csv             # Agent predictions\n\n/private/data/task-id/         # Private evaluation data\n\u2514\u2500\u2500 prepared/private/\n    \u2514\u2500\u2500 answers.csv            # Ground truth (inaccessible to agents)\n</code></pre>"},{"location":"components/environment/#mount-configuration","title":"Mount Configuration","text":"<p>Configured by the agent runner:</p> <pre><code>volumes_config = {\n    # Task data (read-only)\n    str(task.public_dir): {\n        \"bind\": \"/home/data\",\n        \"mode\": \"ro\"\n    },\n    # Private answers (read-only, restricted access)\n    str(task.private_dir): {\n        \"bind\": f\"/private/data/{task.id}/prepared/private/\",\n        \"mode\": \"ro\"\n    }\n}\n</code></pre>"},{"location":"components/environment/#build-process","title":"Build Process","text":""},{"location":"components/environment/#multi-stage-build","title":"Multi-Stage Build","text":"<p>The Dockerfile uses multi-stage builds for efficiency:</p> <pre><code># Stage 1: Base system and conda\nFROM ubuntu:22.04 as base\nRUN apt-get update &amp;&amp; install system packages\nRUN install miniconda\n\n# Stage 2: Biomedical dependencies  \nFROM base as biomedical\nRUN conda install biomedical libraries\nRUN pip install ML frameworks\n\n# Stage 3: Final image\nFROM biomedical as final\nCOPY application files\nRUN final configuration\n</code></pre>"},{"location":"components/environment/#build-optimization","title":"Build Optimization","text":"<p>Caching Strategy: - Layer-wise dependency installation - Separate system and Python packages - Conditional heavy dependency installation</p> <p>Size Optimization: - Multi-stage builds - Cleanup of package managers - Removal of development headers</p> <p>Example Build Command: <pre><code>docker build \\\n    --target final \\\n    --build-arg INSTALL_HEAVY_DEPENDENCIES=true \\\n    --tag biomlbench-env \\\n    -f environment/Dockerfile \\\n    .\n</code></pre></p>"},{"location":"components/environment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"components/environment/#common-issues","title":"Common Issues","text":"<p>Container fails to start: <pre><code># Check Docker daemon\ndocker info\n\n# Verify image exists\ndocker images biomlbench-env\n\n# Check container logs\ndocker logs &lt;container-id&gt;\n</code></pre></p> <p>Permission denied errors: <pre><code># Check volume permissions\nls -la /path/to/mounted/directory\n\n# Verify user mapping\ndocker exec &lt;container&gt; id\n\n# Check SELinux context (Linux)\nls -Z /path/to/mounted/directory\n</code></pre></p> <p>Grading server not responding: <pre><code># Check server health\ndocker exec &lt;container&gt; curl http://localhost:5000/health\n\n# Verify server process\ndocker exec &lt;container&gt; ps aux | grep grading_server\n\n# Check server logs\ndocker exec &lt;container&gt; cat /var/log/grading_server.log\n</code></pre></p>"},{"location":"components/environment/#resource-monitoring","title":"Resource Monitoring","text":"<p>Memory usage: <pre><code># Monitor container memory\ndocker stats &lt;container-id&gt;\n\n# Check OOM kills\ndmesg | grep -i \"killed process\"\n</code></pre></p> <p>Disk usage: <pre><code># Container filesystem usage\ndocker exec &lt;container&gt; df -h\n\n# Docker space usage\ndocker system df\n</code></pre></p> <p>GPU usage (if applicable): <pre><code># Check GPU availability\ndocker exec &lt;container&gt; nvidia-smi\n\n# Monitor GPU utilization\nnvidia-smi -l 1\n</code></pre></p>"},{"location":"components/scripts/","title":"Scripts Reference","text":"<p>The <code>scripts/</code> directory contains automation scripts for building, testing, and managing BioML-bench environments and agents.</p>"},{"location":"components/scripts/#build-scripts","title":"Build Scripts","text":""},{"location":"components/scripts/#build_base_envsh","title":"<code>build_base_env.sh</code>","text":"<p>Builds the foundational Docker image (<code>biomlbench-env</code>) with biomedical libraries and dependencies.</p> <p>Usage: <pre><code>./scripts/build_base_env.sh\n</code></pre></p> <p>Features: - Installs Python 3.11 with conda - Includes biomedical libraries (RDKit, BioPython) - Configures ML frameworks (TensorFlow, PyTorch) - Sets up the grading server environment - Performs post-build validation tests</p> <p>Requirements: - Docker installed and running - Internet connection for downloading dependencies - ~10GB disk space for the image</p> <p>Example Output: <pre><code>\ud83e\uddec Building BioML-bench Base Environment\n=======================================\n\ud83d\udccb Pre-build checks...\n\u2705 environment/Dockerfile\n\u2705 environment/requirements.txt\n\ud83d\udd28 Building biomlbench-env Docker image...\n\u2705 Successfully built biomlbench-env image\n\ud83e\uddea Testing base image...\n\u2705 Python is available\n\u2705 BioML-bench is importable\n\u2705 Biomedical and ML dependencies are available\n\u2705 Agent conda environment is ready\n\ud83c\udf89 Base environment build completed successfully!\n</code></pre></p>"},{"location":"components/scripts/#build_agentsh","title":"<code>build_agent.sh</code>","text":"<p>Builds Docker images for individual agents.</p> <p>Usage: <pre><code>./scripts/build_agent.sh &lt;agent-name&gt;\n\n# Examples\n./scripts/build_agent.sh dummy\n./scripts/build_agent.sh aide\n</code></pre></p> <p>Features: - Builds agent-specific Docker images - Inherits from <code>biomlbench-env</code> base image - Configures agent-specific dependencies - Sets up environment variables and entrypoints</p> <p>Agent Directory Structure: <pre><code>agents/&lt;agent-name&gt;/\n\u251c\u2500\u2500 Dockerfile          # Agent-specific build instructions\n\u251c\u2500\u2500 config.yaml         # Agent configuration\n\u251c\u2500\u2500 start.sh            # Agent execution script\n\u251c\u2500\u2500 requirements.txt    # Additional dependencies\n\u2514\u2500\u2500 src/                # Agent source code\n</code></pre></p>"},{"location":"components/scripts/#build_agent_enhancedsh","title":"<code>build_agent_enhanced.sh</code>","text":"<p>Enhanced agent building with additional features and validation.</p> <p>Usage: <pre><code>./scripts/build_agent_enhanced.sh &lt;agent-name&gt; [options]\n</code></pre></p> <p>Additional Features: - Multi-stage builds for smaller images - Build caching optimization - Security scanning integration - Performance benchmarking - Automated testing of built images</p>"},{"location":"components/scripts/#testing-scripts","title":"Testing Scripts","text":""},{"location":"components/scripts/#test_environmentsh","title":"<code>test_environment.sh</code>","text":"<p>Comprehensive testing of the BioML-bench environment setup.</p> <p>Usage: <pre><code>./scripts/test_environment.sh\n</code></pre></p> <p>Tests Performed: - Docker daemon connectivity - Base image functionality - Agent image building - Container execution - Data mounting and permissions - Network isolation - GPU support (if available)</p> <p>Example Test Sequence: <pre><code># Test Docker setup\ndocker --version\ndocker info\n\n# Test base environment\ndocker run --rm biomlbench-env python --version\n\n# Test agent building\n./scripts/build_agent.sh dummy\n\n# Test agent execution\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n</code></pre></p>"},{"location":"components/scripts/#validate_environmentsh","title":"<code>validate_environment.sh</code>","text":"<p>Validates system prerequisites and configuration.</p> <p>Usage: <pre><code>./scripts/validate_environment.sh\n</code></pre></p> <p>Validation Checks: - Python version compatibility (\u22653.11) - Docker installation and permissions - Available disk space - Memory requirements - GPU support detection - Network connectivity - Required system packages</p> <p>Prerequisites Validation: <pre><code># Check Python version\npython --version  # Should be \u22653.11\n\n# Check Docker\ndocker --version\ndocker run hello-world\n\n# Check disk space\ndf -h  # Need ~15GB free\n\n# Check memory\nfree -h  # Recommend \u22658GB RAM\n</code></pre></p>"},{"location":"components/scripts/#test_agent_pipelinepy","title":"<code>test_agent_pipeline.py</code>","text":"<p>End-to-end testing of the agent execution pipeline.</p> <p>Usage: <pre><code>python scripts/test_agent_pipeline.py [options]\n</code></pre></p> <p>Test Coverage: - Task preparation workflow - Agent container creation - Data volume mounting - Execution environment setup - Submission file generation - Cleanup and resource management</p> <p>Command Options: - <code>--agent</code> - Specific agent to test - <code>--task</code> - Specific task to test - <code>--timeout</code> - Execution timeout - <code>--retain-containers</code> - Keep containers for debugging</p>"},{"location":"components/scripts/#monitoring-scripts","title":"Monitoring Scripts","text":""},{"location":"components/scripts/#monitor_systempy","title":"<code>monitor_system.py</code>","text":"<p>System resource monitoring during agent execution.</p> <p>Usage: <pre><code>python scripts/monitor_system.py [options]\n</code></pre></p> <p>Monitoring Features: - CPU usage tracking - Memory consumption - Disk I/O statistics - GPU utilization (if available) - Container resource limits - Network traffic monitoring</p> <p>Output Options: - Real-time console output - CSV log files - JSON metrics export - Grafana-compatible format</p> <p>Example Usage: <pre><code># Monitor during agent run\npython scripts/monitor_system.py --output metrics.csv &amp;\nbiomlbench run-agent --agent aide --task-id caco2-wang\n</code></pre></p>"},{"location":"components/scripts/#utility-scripts","title":"Utility Scripts","text":""},{"location":"components/scripts/#robust_postprocesspy","title":"<code>robust_postprocess.py</code>","text":"<p>Post-processing and analysis of agent run results.</p> <p>Usage: <pre><code>python scripts/robust_postprocess.py --run-dir &lt;run-directory&gt; [options]\n</code></pre></p> <p>Features: - Submission validation and repair - Log file analysis and summarization - Performance metric extraction - Error categorization and reporting - Resource usage analysis</p> <p>Analysis Outputs: - Summary reports (JSON/HTML) - Performance visualizations - Error logs and diagnostics - Resource consumption charts</p> <p>Command Options: - <code>--run-dir</code> - Target run directory - <code>--output-format</code> - Report format (json/html/csv) - <code>--include-logs</code> - Include detailed log analysis - <code>--validate-submissions</code> - Check submission format compliance</p>"},{"location":"components/scripts/#script-configuration","title":"Script Configuration","text":""},{"location":"components/scripts/#environment-variables","title":"Environment Variables","text":"<p>Scripts respect these environment variables:</p> <pre><code># Build configuration\nexport DOCKER_BUILDKIT=1                    # Enable BuildKit\nexport BUILD_PARALLEL=4                     # Parallel build jobs\n\n# Testing configuration  \nexport TEST_TIMEOUT=3600                    # Test timeout (seconds)\nexport RETAIN_TEST_CONTAINERS=false         # Keep test containers\n\n# Resource limits\nexport MAX_MEMORY=\"8g\"                      # Container memory limit\nexport MAX_CPUS=\"4\"                         # CPU limit\n\n# Monitoring configuration\nexport METRICS_INTERVAL=5                   # Monitoring interval (seconds)\nexport METRICS_OUTPUT=\"/tmp/metrics\"        # Metrics output directory\n</code></pre>"},{"location":"components/scripts/#build-options","title":"Build Options","text":"<p>Most build scripts support common options:</p> <pre><code># Build with custom base image\n./scripts/build_agent.sh dummy --base-image custom-biomlbench-env\n\n# Skip dependency installation\n./scripts/build_agent.sh aide --skip-deps\n\n# Enable debug builds\n./scripts/build_agent.sh aide --debug\n\n# Use build cache\n./scripts/build_agent.sh aide --cache\n\n# Multi-platform builds\n./scripts/build_agent.sh aide --platform linux/amd64,linux/arm64\n</code></pre>"},{"location":"components/scripts/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"components/scripts/#initial-setup","title":"Initial Setup","text":"<pre><code># Complete environment setup\n./scripts/validate_environment.sh\n./scripts/build_base_env.sh\n./scripts/test_environment.sh\n\n# Build all agents\nfor agent in dummy aide mlagentbench; do\n    ./scripts/build_agent.sh $agent\ndone\n</code></pre>"},{"location":"components/scripts/#development-workflow","title":"Development Workflow","text":"<pre><code># Build and test specific agent\n./scripts/build_agent.sh my-agent --debug\npython scripts/test_agent_pipeline.py --agent my-agent --task caco2-wang\n\n# Monitor resource usage\npython scripts/monitor_system.py --agent my-agent &amp;\nbiomlbench run-agent --agent my-agent --task-id caco2-wang\n\n# Analyze results\npython scripts/robust_postprocess.py --run-dir runs/latest-run-group/\n</code></pre>"},{"location":"components/scripts/#continuous-integration","title":"Continuous Integration","text":"<pre><code># CI/CD pipeline\n./scripts/validate_environment.sh || exit 1\n./scripts/build_base_env.sh || exit 1\n./scripts/test_environment.sh || exit 1\n\n# Test all agents\nfor agent in $(ls agents/*/config.yaml | cut -d/ -f2); do\n    ./scripts/build_agent.sh $agent || exit 1\n    python scripts/test_agent_pipeline.py --agent $agent --timeout 600 || exit 1\ndone\n</code></pre>"},{"location":"components/scripts/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Build with optimizations\nexport DOCKER_BUILDKIT=1\nexport BUILD_PARALLEL=8\n\n# Use build cache effectively\n./scripts/build_agent.sh aide --cache --parallel\n\n# Monitor and optimize\npython scripts/monitor_system.py --profile-memory --profile-cpu\n</code></pre>"},{"location":"components/scripts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"components/scripts/#common-issues","title":"Common Issues","text":"<p>Build failures: <pre><code># Clean Docker state\ndocker system prune -a\n\n# Rebuild base image\n./scripts/build_base_env.sh --no-cache\n\n# Check disk space\ndf -h\n</code></pre></p> <p>Test failures: <pre><code># Run validation first\n./scripts/validate_environment.sh\n\n# Check Docker permissions\ndocker run hello-world\n\n# Verify Python environment\npython -c \"import biomlbench; print('OK')\"\n</code></pre></p> <p>Resource issues: <pre><code># Monitor system resources\npython scripts/monitor_system.py --real-time\n\n# Adjust container limits\nexport MAX_MEMORY=\"16g\"\nexport MAX_CPUS=\"8\"\n</code></pre></p>"},{"location":"components/scripts/#script-dependencies","title":"Script Dependencies","text":"<p>Most scripts require: - Bash shell (Linux/macOS) - Docker (version \u226520.10) - Python \u22653.11 - Internet connectivity - Sufficient disk space (\u226515GB)</p> <p>For GPU support: - NVIDIA drivers - NVIDIA Container Toolkit - CUDA-compatible hardware </p>"},{"location":"components/task_development/","title":"Task Development","text":"<p>Guide for creating new biomedical benchmark tasks in BioML-bench.</p>"},{"location":"components/task_development/#task-structure","title":"Task Structure","text":"<p>Each task requires:</p> <ul> <li><code>config.yaml</code> - Task configuration and metadata</li> <li><code>prepare.py</code> - Data preparation logic</li> <li><code>grade.py</code> - Evaluation function</li> <li><code>description.md</code> - Task description for agents</li> </ul>"},{"location":"components/task_development/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Create task directory:    <pre><code>mkdir -p biomlbench/tasks/my-new-task\n</code></pre></p> </li> <li> <p>Add configuration file</p> </li> <li>Implement preparation logic</li> <li>Define evaluation metrics</li> <li>Test with dummy agent</li> </ol> <p>See Adding Tasks Guide for detailed instructions. </p>"},{"location":"developer/adding_tasks/","title":"Adding Tasks","text":"<p>Detailed guide for adding new biomedical benchmark tasks to BioML-bench.</p>"},{"location":"developer/adding_tasks/#task-requirements","title":"Task Requirements","text":"<p>Every task needs: - Biomedical relevance and scientific validity - Clear evaluation metrics - Sufficient data for training and testing - Proper train/test splits to prevent data leakage</p>"},{"location":"developer/adding_tasks/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Create task directory structure</li> <li>Configure task metadata</li> <li>Implement data preparation</li> <li>Define evaluation logic</li> <li>Write task description</li> <li>Test and validate</li> </ol>"},{"location":"developer/adding_tasks/#task-configuration-configyaml","title":"Task Configuration (<code>config.yaml</code>)","text":"<pre><code>id: my-biomedical-task\nname: \"My Biomedical Task\"\ntask_type: drug_discovery  # or medical_imaging, protein_engineering\ndomain: pharmacokinetics   # specific biomedical domain\ndifficulty: medium         # easy, medium, hard\n\ndata_source:\n  type: kaggle            # or polaris, custom\n  competition_id: my-task\n\ndataset:\n  answers: my-task/prepared/private/answers.csv\n  sample_submission: my-task/prepared/public/sample_submission.csv\n\ngrader:\n  name: rmse\n  grade_fn: biomlbench.tasks.my-task.grade:grade\n\npreparer: biomlbench.tasks.my-task.prepare:prepare\n\nbiomedical_metadata:\n  modality: \"molecular_properties\"\n  organ_system: \"liver\"\n  data_type: \"regression\"\n  clinical_relevance: \"drug_metabolism\"\n</code></pre>"},{"location":"developer/adding_tasks/#data-preparation-preparepy","title":"Data Preparation (<code>prepare.py</code>)","text":"<pre><code>from pathlib import Path\nimport pandas as pd\n\ndef prepare(task_dir: Path, raw_dir: Path, public_dir: Path, private_dir: Path) -&gt; Path:\n    \"\"\"Prepare task data with train/test splits.\"\"\"\n\n    # Download and process raw data\n    # Create train.csv, test_features.csv, sample_submission.csv\n    # Generate private answers.csv\n\n    return public_dir\n</code></pre>"},{"location":"developer/adding_tasks/#evaluation-logic-gradepy","title":"Evaluation Logic (<code>grade.py</code>)","text":"<pre><code>import pandas as pd\nimport numpy as np\n\ndef grade(submission: pd.DataFrame, answers: pd.DataFrame) -&gt; float:\n    \"\"\"Calculate task-specific metric.\"\"\"\n\n    y_true = answers['target'].values\n    y_pred = submission['prediction'].values\n\n    # Implement domain-specific metric\n    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n</code></pre>"},{"location":"developer/adding_tasks/#testing-new-tasks","title":"Testing New Tasks","text":"<pre><code># Test preparation\nbiomlbench prepare -t my-task\n\n# Test with dummy agent\nbiomlbench run-agent --agent dummy --task-id my-task\n\n# Validate submission\nbiomlbench grade-sample submission.csv my-task\n</code></pre>"},{"location":"developer/architecture/","title":"Architecture Overview","text":"<p>BioML-bench follows a modular architecture designed for extensibility and security.</p>"},{"location":"developer/architecture/#core-components","title":"Core Components","text":""},{"location":"developer/architecture/#registry-system","title":"Registry System","text":"<ul> <li>Tasks - Discovery and management of biomedical benchmark tasks</li> <li>Agents - Registration and configuration of AI agents</li> <li>Data Sources - Pluggable data acquisition from multiple repositories</li> </ul>"},{"location":"developer/architecture/#execution-engine","title":"Execution Engine","text":"<ul> <li>Containerization - Docker-based secure execution environment</li> <li>Parallel Processing - Multi-worker task execution</li> <li>Resource Management - Memory, CPU, and GPU allocation</li> </ul>"},{"location":"developer/architecture/#evaluation-framework","title":"Evaluation Framework","text":"<ul> <li>Domain-Specific Metrics - Biomedical evaluation functions</li> <li>Human Baselines - Expert performance comparisons</li> <li>Medal System - Kaggle-style performance ranking</li> </ul>"},{"location":"developer/architecture/#security-model","title":"Security Model","text":""},{"location":"developer/architecture/#container-isolation","title":"Container Isolation","text":"<ul> <li>Network isolation - No external network access during execution</li> <li>Filesystem isolation - Read-only data access, private answer protection</li> <li>User isolation - Non-root execution, limited capabilities</li> </ul>"},{"location":"developer/architecture/#data-protection","title":"Data Protection","text":"<ul> <li>Private/public splits - Clear separation of training and evaluation data</li> <li>Checksum validation - Data integrity verification</li> <li>Access controls - Restricted access to ground truth labels</li> </ul>"},{"location":"developer/architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Task Preparation - Download and process datasets</li> <li>Agent Execution - Run agents in isolated containers</li> <li>Submission Collection - Extract predictions and logs</li> <li>Evaluation - Score submissions against private answers</li> <li>Reporting - Generate comprehensive evaluation reports</li> </ol>"},{"location":"developer/architecture/#extension-points","title":"Extension Points","text":""},{"location":"developer/architecture/#adding-new-components","title":"Adding New Components","text":"<ul> <li>Tasks - Implement prepare/grade functions</li> <li>Agents - Create Docker containers with standard interface</li> <li>Data Sources - Implement DataSource interface</li> <li>Metrics - Define custom evaluation functions</li> </ul>"},{"location":"developer/architecture/#configuration-system","title":"Configuration System","text":"<ul> <li>YAML-based - Human-readable configuration files</li> <li>Environment variables - Runtime configuration options</li> <li>Container configs - Docker resource and security settings </li> </ul>"},{"location":"developer/contributing/","title":"Contributing to BioML-bench","text":"<p>We welcome contributions to BioML-bench! This guide covers how to contribute effectively.</p>"},{"location":"developer/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork and clone the repository</li> <li>Install development dependencies: <pre><code>uv sync --extra dev\n</code></pre></li> <li>Build the base environment: <pre><code>./scripts/build_base_env.sh\n</code></pre></li> </ol>"},{"location":"developer/contributing/#types-of-contributions","title":"Types of Contributions","text":"<ul> <li>New biomedical tasks - Add tasks from your domain expertise</li> <li>Agent implementations - Contribute new AI agents</li> <li>Data source integrations - Support new biomedical databases</li> <li>Documentation improvements - Help improve this documentation</li> <li>Bug fixes and optimizations - Code quality improvements</li> </ul>"},{"location":"developer/contributing/#contribution-process","title":"Contribution Process","text":"<ol> <li>Create a feature branch</li> <li>Make your changes</li> <li>Test thoroughly with existing tasks</li> <li>Submit a pull request</li> <li>Address review feedback</li> </ol>"},{"location":"developer/contributing/#code-standards","title":"Code Standards","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Add comprehensive docstrings</li> <li>Include type hints</li> <li>Write tests for new functionality</li> <li>Update documentation for user-facing changes</li> </ul>"},{"location":"developer/contributing/#testing","title":"Testing","text":"<p>Test your changes with: <pre><code># Run basic tests\n./scripts/test_environment.sh\n\n# Test specific agent\nbiomlbench run-agent --agent dummy --task-id caco2-wang\n\n# Validate new tasks\nbiomlbench prepare -t my-new-task\n</code></pre></p>"},{"location":"developer/creating_agents/","title":"Creating Agents","text":"<p>Guide for developing custom AI agents for BioML-bench.</p>"},{"location":"developer/creating_agents/#agent-requirements","title":"Agent Requirements","text":"<p>LLM agents must:</p> <ul> <li>Run autonomously in Docker containers without human intervention</li> <li>Read and comprehend biomedical task descriptions from <code>/home/data/description.md</code></li> <li>Analyze data and design appropriate ML approaches</li> <li>Implement and execute complete ML pipelines (preprocessing, training, evaluation)</li> <li>Generate predictions and write them to <code>/home/submission/submission.csv</code></li> <li>Handle domain-specific data (SMILES for molecules, medical images, protein sequences, etc.)</li> </ul> <p>Unlike traditional ML models, agents must solve tasks from scratch using only the task description and training data.</p>"},{"location":"developer/creating_agents/#agent-structure","title":"Agent Structure","text":"<pre><code>agents/my-agent/\n\u251c\u2500\u2500 Dockerfile           # Container build instructions\n\u251c\u2500\u2500 config.yaml         # Agent configuration\n\u251c\u2500\u2500 start.sh            # Execution entry point\n\u251c\u2500\u2500 requirements.txt    # Python dependencies\n\u2514\u2500\u2500 src/                # Agent source code\n    \u2514\u2500\u2500 agent.py\n</code></pre>"},{"location":"developer/creating_agents/#configuration-configyaml","title":"Configuration (<code>config.yaml</code>)","text":"<pre><code>my-agent:\n  start: my-agent/start.sh\n  dockerfile: my-agent/Dockerfile\n  env_vars:\n    OPENAI_API_KEY: \"${OPENAI_API_KEY}\"\n  privileged: false  # Set true only if necessary\n</code></pre>"},{"location":"developer/creating_agents/#docker-setup-dockerfile","title":"Docker Setup (<code>Dockerfile</code>)","text":"<pre><code>FROM biomlbench-env\n\n# Copy agent code\nCOPY src/ /home/agent/src/\nCOPY requirements.txt /home/agent/\n\n# Install dependencies\nWORKDIR /home/agent\nRUN /opt/conda/bin/conda run -n agent pip install -r requirements.txt\n\n# Set working directory\nWORKDIR /home\n</code></pre>"},{"location":"developer/creating_agents/#agent-interface","title":"Agent Interface","text":"<p>Agents are LLM-based systems that autonomously solve biomedical ML tasks end-to-end. They must:</p> <ol> <li>Understand the task by reading the description and analyzing the data</li> <li>Design an ML approach appropriate for the biomedical problem  </li> <li>Implement, train, and evaluate models</li> <li>Generate predictions for submission</li> </ol>"},{"location":"developer/creating_agents/#example-agent-structure","title":"Example Agent Structure","text":"<pre><code># src/agent.py\nimport pandas as pd\nfrom pathlib import Path\n\ndef main():\n    # 1. Read and understand the task\n    with open('/home/data/description.md', 'r') as f:\n        task_description = f.read()\n        # Agent parses task type, evaluation metric, clinical context\n\n    # 2. Analyze the data\n    train_df = pd.read_csv('/home/data/train.csv')\n    test_df = pd.read_csv('/home/data/test_features.csv')\n    sample_submission = pd.read_csv('/home/data/sample_submission.csv')\n\n    # 3. ... Agent Logic ...\n\n    # 4. Create submission in required format\n    submission = pd.DataFrame({\n        'id': test_df['id'],\n        'prediction': predictions  # Column name from sample_submission\n    })\n\n    submission.to_csv('/home/submission/submission.csv', index=False)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"developer/creating_agents/#agent-input-data","title":"Agent Input Data","text":"<ul> <li><code>description.md</code>: Complete task description including:</li> <li>Task type (regression, classification, segmentation)</li> <li>Data format and features</li> <li>Evaluation metric</li> <li> <p>Baseline approaches and references</p> </li> <li> <p><code>train.csv</code>: Training data with features and targets</p> </li> <li><code>test_features.csv</code>: Test features (no targets)</li> <li><code>sample_submission.csv</code>: Expected submission format</li> </ul>"},{"location":"developer/creating_agents/#building-and-testing","title":"Building and Testing","text":"<pre><code># Build agent\n./scripts/build_agent.sh my-agent # e.g., ./scripts/build_agent.sh aide\n\n# Test agent\nbiomlbench run-agent --agent my-agent --task-id caco2-wang # e.g., biomlbench run-agent --agent aide --task-id caco2-wang\n</code></pre>"}]}